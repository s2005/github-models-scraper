[
  {
    "id": "o3-mini",
    "registry": "azure-openai",
    "name": "OpenAI o3-mini",
    "original_name": "o3-mini",
    "friendly_name": "OpenAI o3-mini",
    "task": "chat-completion",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "#### o1 and o3 Series Models: Enhanced Reasoning and Problem Solving on Azure\n\nThe o1 and o3 series models are specifically designed to tackle reasoning and problem-solving tasks with increased focus and capability. These models spend more time processing and understanding the user's request, making them exceptionally strong in areas like science, coding, math and similar fields. For example, o1 can be used by healthcare researchers to annotate cell sequencing data, by physicists to generate complicated mathematical formulas needed for quantum optics, and by developers in all fields to build and execute multi-step workflows.\n\n## Key Capabilities of these models\n\n- o1 added advanced image analysis capabilities with the new version. Enhance your prompts and context with images for additional insights.\n- o3-mini follows o1 mini but adds the features supported by o1 like function calling and tools.\n- Complex Code Generation: Capable of generating algorithms and handling advanced coding tasks to support developers.\n- Advanced Problem Solving: Ideal for comprehensive brainstorming sessions and addressing multifaceted challenges.\n- Complex Document Comparison: Perfect for analyzing contracts, case files, or legal documents to identify subtle differences.\n- Instruction Following and Workflow Management: Particularly effective for managing workflows requiring shorter contexts.\n\n## Features and properties supported in o3-mini model\n\n- Supports both System message and the new Developer message to improve upgrade experience.\n- Reasoning effort as in `high`, `medium`, and `low`. It controls whether the model thinks \"less\" or \"more\" in terms of applying cognitive reasoning.\n- Structured outputs and functions/tools.\n- Context window: 200K, Max Completion Tokens: 100K\n\n## Model Variants\n\n- `o3-mini`: Now includes the o1 features with significant cost-efficiencies for scenarios requiring high performance.\n- `o1`: The most capable model in the o1 series, offering enhanced reasoning abilities. Now generally available.\n- `o1-mini`: A faster and more cost-efficient option in the o1 series, ideal for coding tasks requiring speed and lower resource consumption.\n\n## Limitations\n\no1 model does not include all the features available in other models. ",
    "summary": "o3-mini includes the o1 features with significant cost-efficiencies for scenarios requiring high performance.",
    "model_family": "OpenAI",
    "model_version": "2025-01-31",
    "notes": "## Model provider\n\nThis model is provided through the Azure OpenAI Service.\n\n## Relevant documents\n\nThe following documents are applicable:\n\n- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/legal/cognitive-services/openai/overview)\n- [Transparency Note for Azure OpenAI Service](https://learn.microsoft.com/legal/cognitive-services/openai/transparency-note)\n\n## Safety\n\nOpenAI has incorporated additional safety measures into the o1 models, including new techniques to help the models refuse unsafe requests. These advancements make the o1 series some of the most robust models available.\n\nOpenAI measures safety is by testing how well models continue to follow its safety rules if a user tries to bypass them (known as \"jailbreaking\"). In OpenAI\u2019s internal tests, GPT-4o scored 22 (on a scale of 0-100) while o1-preview model scored 84. You can read more about this in the OpenAI\u2019s [system card](https://cdn.openai.com/o1-system-card-20241205.pdf) and [research post](https://openai.com/index/learning-to-reason-with-llms/).",
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "rate_limit_tier": "custom",
    "supported_languages": [
      "en",
      "it",
      "af",
      "es",
      "de",
      "fr",
      "id",
      "ru",
      "pl",
      "uk",
      "el",
      "lv",
      "zh",
      "ar",
      "tr",
      "ja",
      "sw",
      "cy",
      "ko",
      "is",
      "bn",
      "ur",
      "ne",
      "th",
      "pa",
      "mr",
      "te"
    ],
    "max_output_tokens": 100000,
    "max_input_tokens": 200000,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/o3-mini",
    "page": 1
  },
  {
    "id": "DeepSeek-R1",
    "registry": "azureml-deepseek",
    "name": "DeepSeek-R1",
    "original_name": "DeepSeek-R1",
    "friendly_name": "DeepSeek-R1",
    "task": "chat-completion",
    "publisher": "DeepSeek",
    "license": "MIT",
    "description": "*Learn more: \\[[original model announcement](https://github.com/deepseek-ai/DeepSeek-R1/tree/main)\\]*\n\nDeepSeek-R1 excels at reasoning tasks using a step-by-step training process, such as language, scientific reasoning, and coding tasks. It features 671B total parameters with 37B active parameters, and 128k context length.\n\nDeepSeek-R1 builds on the progress of earlier reasoning-focused models that improved performance by extending Chain-of-Thought (CoT) reasoning. DeepSeek-R1 takes things further by combining reinforcement learning (RL) with fine-tuning on carefully chosen datasets. It evolved from an earlier version, DeepSeek-R1-Zero, which relied solely on RL and showed strong reasoning skills but had issues like hard-to-read outputs and language inconsistencies. To address these limitations, DeepSeek-R1 incorporates a small amount of cold-start data and follows a refined training pipeline that blends reasoning-oriented RL with supervised fine-tuning on curated datasets, resulting in a model that achieves state-of-the-art performance on reasoning benchmarks.\n\n### Usage Recommendations\n\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\n\n- Avoid adding a system prompt; all instructions should be contained within the user prompt.\n- For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n- When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n### Additional recommendations\n\nThe model's reasoning output (contained within the `<think>` tags) may contain more harmful content than the model's final response. Consider how your application will use or display the reasoning output; you may want to suppress the reasoning output in a production setting.\n",
    "summary": "DeepSeek-R1 excels at reasoning tasks using a step-by-step training process, such as language, scientific reasoning, and coding tasks.",
    "model_family": "DeepSeek",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).\n",
    "tags": [
      "reasoning",
      "coding",
      "agents"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "zh"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 128000,
    "training_data_date": "",
    "evaluation": "> *Extract from the [original model evaluation](https://github.com/deepseek-ai/DeepSeek-R1/tree/main?tab=readme-ov-file#4-evaluation-results)*\n\nFor DeepSeek-R1, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, a temperature of `0.6`, a top-p value of `0.95`, and generate 64 responses per query to estimate pass@1 were used.\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |",
    "license_description": "The DeepSeek R1 model is provided subject to the MIT License. Your use of the model is subject to the MIT License and, when used in Azure, applicable Azure product terms.\n \nCopyright (c) 2023 DeepSeek\n \nMIT License\n \nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n \nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n \nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-deepseek/DeepSeek-R1",
    "page": 1
  },
  {
    "id": "Phi-4",
    "registry": "azureml",
    "name": "Phi-4",
    "original_name": "Phi-4",
    "friendly_name": "Phi-4",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "MIT",
    "description": "Phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.\n\nPhi-4 underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nFor more information, reference the [Phi-4 Technical Report](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/P4TechReport.pdf).\n\n### Model Architecture\n\nPhi-4 is a 14B parameters, dense decoder-only transformer model. \n\n### Training Data\n\nOur training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\n\n1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\n\n2. Newly created synthetic, \"textbook-like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\n\n3. Acquired academic books and Q&A datasets.\n\n4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.\n",
    "summary": "Phi-4 14B, a highly capable model for low latency scenarios.",
    "model_family": "Microsoft",
    "model_version": "3",
    "notes": "## Intended Use\n\n### Primary Use Cases\n\nOur model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n1. Memory/compute constrained environments.\n2. Latency bound scenarios.\n3. Reasoning and logic.\n\n### Out-of-Scope Use Cases\nOur models is not specifically designed or evaluated for all downstream purposes, thus:\n\n1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model\u2019s focus on English.\n3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n## Safety \n\n### Approach \n\nPhi-4 has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated synthetic datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. \n\n### Safety Evaluation and Red-Teaming \n\nPrior to release, Phi-4 followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the independent AI Red Team (AIRT) at Microsoft to assess safety risks posed by `phi-4` in both average and adversarial user scenarios. In the average user scenario, AIRT emulated typical single-turn and multi-turn interactions to identify potentially risky behaviors. The adversarial user scenario tested a wide range of techniques aimed at intentionally subverting the model\u2019s safety training including jailbreaks, encoding-based attacks, multi-turn attacks, and adversarial suffix attacks.   \n\nPlease refer to the technical report for more details on safety alignment. \n\n## Responsible AI Considerations\n\nLike other language models, `phi-4` can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n\n* **Quality of Service:** The model is trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. `phi-4` is not intended to support multilingual use. \n\n* **Representation of Harms & Perpetuation of Stereotypes:** These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.  \n\n* **Inappropriate or Offensive Content:** These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.  \n\n* **Information Reliability:** Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n\n* **Limited Scope for Code:** Majority of `phi-4` training data is based in Python and uses common packages such as `typing`, `math`, `random`, `collections`, `datetime`, `itertools`. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.  \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Using safety services like [Azure AI Content Safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety) that have advanced guardrails is highly recommended. Important areas for consideration include:\n\n* **Allocation:** Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. \n\n* **High-Risk Scenarios:** Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.  \n\n* **Misinformation:** Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).    \n\n* **Generation of Harmful Content:** Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.  \n\n* **Misuse:** Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n",
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "ar",
      "bn",
      "cs",
      "da",
      "de",
      "el",
      "es",
      "fa",
      "fi",
      "fr",
      "gu",
      "ha",
      "he",
      "hi",
      "hu",
      "id",
      "it",
      "ja",
      "jv",
      "kn",
      "ko",
      "ml",
      "mr",
      "nl",
      "no",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "uk",
      "ur",
      "vi",
      "yo",
      "zh"
    ],
    "max_output_tokens": 16384,
    "max_input_tokens": 16384,
    "training_data_date": "Jun 2024",
    "evaluation": "We evaluated `phi-4` using [OpenAI\u2019s SimpleEval](https://github.com/openai/simple-evals) and our own internal benchmarks to understand the model\u2019s capabilities, more specifically: \n\n* **MMLU:** Popular aggregated dataset for multitask language understanding.\n\n* **MATH:** Challenging competition math problems.\n\n* **GPQA:** Complex, graduate-level science questions.\n\n* **DROP:** Complex comprehension and reasoning.\n\n* **MGSM:** Multi-lingual grade-school math.\n\n* **HumanEval:** Functional code generation.\n\n* **SimpleQA:** Factual responses.\n\nTo understand the capabilities, we compare `phi-4` with a set of models over OpenAI\u2019s SimpleEval benchmark. \n\nAt the high-level overview of the model quality on representative benchmarks. For the table below, higher numbers indicate better performance: \n\n| **Category**                 | **Benchmark** | **phi-4** (14B) | **phi-3** (14B) | **Qwen 2.5** (14B instruct) | **GPT-4o-mini** | **Llama-3.3** (70B instruct) | **Qwen 2.5** (72B instruct) | **GPT-4o** |\n|------------------------------|---------------|-----------|-----------------|----------------------|----------------------|--------------------|-------------------|-----------------|\n| Popular Aggregated Benchmark | MMLU          | 84.8      | 77.9            | 79.9                 | 81.8                 | 86.3               | 85.3              | **88.1**            |\n| Science                      | GPQA          | **56.1**      | 31.2            | 42.9                 | 40.9                 | 49.1               | 49.0              | 50.6            |\n| Math                         | MGSM<br>MATH  | 80.6<br>**80.4** | 53.5<br>44.6 | 79.6<br>75.6 | 86.5<br>73.0 | 89.1<br>66.3* | 87.3<br>80.0              | **90.4**<br>74.6            |\n| Code Generation              | HumanEval     | 82.6      | 67.8            | 72.1                 | 86.2                 | 78.9*               | 80.4              | **90.6**            |\n| Factual Knowledge            | SimpleQA      | 3.0       | 7.6            | 5.4                 | 9.9                  | 20.9               | 10.2              | **39.4**             |\n| Reasoning                    | DROP          | 75.5      | 68.3            | 85.5                 | 79.3                 | **90.2**               | 76.7              | 80.9            |\n\n\\* These scores are lower than those reported by Meta, perhaps because simple-evals has a strict formatting requirement that Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3-70B.\n",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-4",
    "page": 1
  },
  {
    "id": "Codestral-2501",
    "registry": "azureml-mistral",
    "name": "Codestral 25.01",
    "original_name": "Codestral-2501",
    "friendly_name": "Codestral 25.01",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "\nCodestral 25.01 is explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and can also converse in a variety of languages, it can be used to design advanced AI applications for software developers.\n\nA model fluent in 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. \nImprove developers productivity and reduce errors: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. \n\n**New standard on the performance/latency space with a 256k context window.**\n\n## Use-cases\n\n- _Code generation_: code completion, suggestions, translation\n- _Code understanding and documentation_: code summarization and explanation\n- _Code quality_: code review, refactoring, bug fixing and test case generation\n- _Code generation with fill-in-the-middle (FIM) completion_: users can define the starting point of the code using a prompt, and the ending point of the code using an optional suffix and an optional stop. The Codestral model will then generate the code that fits in between, making it ideal for tasks that require a specific piece of code to be generated. ",
    "summary": "Codestral 25.01 by Mistral AI is designed for code generation, supporting 80+ programming languages, and optimized for tasks like code completion and fill-in-the-middle",
    "model_family": "Mistral AI",
    "model_version": "2",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "coding"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 256000,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Codestral-2501",
    "page": 1
  },
  {
    "id": "o1",
    "registry": "azure-openai",
    "name": "OpenAI o1",
    "original_name": "o1",
    "friendly_name": "OpenAI o1",
    "task": "chat-completion",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "#### o1 Series Models: Enhanced Reasoning and Problem Solving on Azure\n\nThe o1 series models are specifically designed to tackle reasoning and problem-solving tasks with increased focus and capability. These models spend more time processing and understanding the user's request, making them exceptionally strong in areas like science, coding, math and similar fields. For example, o1 can be used by healthcare researchers to annotate cell sequencing data, by physicists to generate complicated mathematical formulas needed for quantum optics, and by developers in all fields to build and execute multi-step workflows.\n\n## Key Capabilities of the o1 Series\n\n- [New!] o1 adds advanced image analysis capabilities with the new version. Enhance your prompts and context with images for additional insights.\n- Complex Code Generation: Capable of generating algorithms and handling advanced coding tasks to support developers.\n- Advanced Problem Solving: Ideal for comprehensive brainstorming sessions and addressing multifaceted challenges.\n- Complex Document Comparison: Perfect for analyzing contracts, case files, or legal documents to identify subtle differences.\n- Instruction Following and Workflow Management: Particularly effective for managing workflows requiring shorter contexts.\n\n## Features and properties supported in o1 GA model\n\n- Developer message replaces system message\n- Reasoning effort as in `high`, `medium`, and `low`. It controls whether the model thinks \"less\" or \"more\" in terms of applying cognitive reasoning.\n- Vision input, structured outputs, and tools.\n\n## Model Variants\n\n- `o1`: The most capable model in the o1 series, offering enhanced reasoning abilities. Now generally available.\n- `o1-mini`: A faster and more cost-efficient option in the o1 series, ideal for coding tasks requiring speed and lower resource consumption.\n\n## Limitations\n\no1 model does not include all the features available in other models. ",
    "summary": "Focused on advanced reasoning and solving complex problems, including math and science tasks. Ideal for applications that require deep contextual understanding and agentic workflows.",
    "model_family": "OpenAI",
    "model_version": "2024-12-17",
    "notes": "## Model provider\n\nThis model is provided through the Azure OpenAI Service.\n\n## Relevant documents\n\nThe following documents are applicable:\n\n- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/legal/cognitive-services/openai/overview)\n- [Transparency Note for Azure OpenAI Service](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note)\n\n## Safety\n\nOpenAI has incorporated additional safety measures into the o1 models, including new techniques to help the models refuse unsafe requests. These advancements make the o1 series some of the most robust models available.\n\nOpenAI measures safety is by testing how well models continue to follow its safety rules if a user tries to bypass them (known as \"jailbreaking\"). In OpenAI\u2019s internal tests, GPT-4o scored 22 (on a scale of 0-100) while o1-preview model scored 84. You can read more about this in the OpenAI\u2019s [system card](https://cdn.openai.com/o1-system-card-20241205.pdf) and [research post](https://openai.com/index/learning-to-reason-with-llms/).",
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "rate_limit_tier": "custom",
    "supported_languages": [
      "en",
      "it",
      "af",
      "es",
      "de",
      "fr",
      "id",
      "ru",
      "pl",
      "uk",
      "el",
      "lv",
      "zh",
      "ar",
      "tr",
      "ja",
      "sw",
      "cy",
      "ko",
      "is",
      "bn",
      "ur",
      "ne",
      "th",
      "pa",
      "mr",
      "te"
    ],
    "max_output_tokens": 100000,
    "max_input_tokens": 200000,
    "training_data_date": "Oct 2023",
    "evaluation": "",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/o1",
    "page": 1
  },
  {
    "id": "Mistral-Large-2411",
    "registry": "azureml-mistral",
    "name": "Mistral Large 24.11",
    "original_name": "Mistral-Large-2411",
    "friendly_name": "Mistral Large 24.11",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "Mistral Large 24.11 is an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge and coding capabilities.\n\n**NEW FEATURES.** \n\n- **SYSTEM PROMPTS**\n- **BETTER PERFORMANCE ON LONG CONTEXT**\n- **IMPROVED FUNCTION CALLING**\n\n**Multi-lingual by design.**\u00a0Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish\n\n**Proficient in coding.** Trained on 80+ coding languages such as Python, Java, C, C++, JavaScript, and Bash. Also trained on more specific languages such as Swift and Fortran\n\n**Agent-centric.** Best-in-class agentic capabilities with native function calling and JSON outputting \n\n**Advanced Reasoning.**\u00a0State-of-the-art mathematical and reasoning capabilities\n\n*Context length:*\u00a0128K tokens\n\n*Input:*\u00a0Models input text only.\n\n*Output:*\u00a0Models generate text only.",
    "summary": "Mistral Large 24.11 offers enhanced system prompts, advanced reasoning and function calling capabilities.",
    "model_family": "Mistral AI",
    "model_version": "2",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "rag",
      "agents"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "zh",
      "ja",
      "ko",
      "pt",
      "nl",
      "pl"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 128000,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Mistral-Large-2411",
    "page": 1
  },
  {
    "id": "Llama-3.3-70B-Instruct",
    "registry": "azureml-meta",
    "name": "Llama-3.3-70B-Instruct",
    "original_name": "Llama-3.3-70B-Instruct",
    "friendly_name": "Llama-3.3-70B-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Built with Llama**\n\n**Model Architecture:** Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.3 (text only)  | A new mix of publicly available online data. | 70B | Multilingual Text | Multilingual Text and code  | 128k | Yes | 15T+* | December 2023 |\n\n*Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n",
    "summary": "Llama 3.3 70B Instruct offers enhanced reasoning, math, and instruction following with performance comparable to Llama 3.1 405B.",
    "model_family": "Meta",
    "model_version": "4",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.3 was pretrained on \\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n \n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n### Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n#### Llama 3.3 instruct\n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone**\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\n\n#### Llama 3.3 systems\n\n**Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n#### Capability Specific Considerations\n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n\n**Multilinguality**: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\n\n### Critical and other risks\n\n### We specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of the Llama 3.3 model could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n### **2\\. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber attack enablement**\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\n### Community\n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "tags": [
      "conversation"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 128000,
    "training_data_date": "Dec 2023",
    "evaluation": "\n\nIn this section, we report the results for Llama 3.3 relative to our previous models.\n\n### Instruction tuned models\n\n##\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n|  | MMLU Pro (CoT) | 5 | macro\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n| Math | MATH (CoT) | 0 | sympy\\_intersection\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\n| Tool Use | BFCL v2 | 0 | overall\\_ast\\_summary/macro\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n\n##",
    "license_description": "A custom commercial license, the Llama 3.3 Community License Agreement, is available at <https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE>. If you access or use Llama 3.3, you agree to its Acceptable Use Policy. The most recent copy of this policy can be found at <https://www.llama.com/llama3_3/use-policy>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Llama-3-3-70B-Instruct",
    "page": 1
  },
  {
    "id": "jais-30b-chat",
    "registry": "azureml-core42",
    "name": "JAIS 30b Chat",
    "original_name": "jais-30b-chat",
    "friendly_name": "JAIS 30b Chat",
    "task": "chat-completion",
    "publisher": "Core42",
    "license": "custom",
    "description": "JAIS 30b Chat from Core42 is an auto-regressive bi-lingual LLM for **Arabic** & **English** with state-of-the-art capabilities in Arabic.\n\n## Model Architecture\n\nThe model is based on transformer-based decoder-only (GPT-3) architecture and uses SwiGLU non-linearity. It uses LiBi position embeddings, enabling the model to extrapolate to long sequence lengths, providing improved context length handling. The tuned versions use supervised fine-tuning (SFT).\n\n## Training Datasets\n\nThe pretraining data for Jais-30b is a total of 1.63 T tokens consisting of English, Arabic, and code. Jais-30b-chat model is finetuned with both Arabic and English prompt-response pairs. We extended our finetuning datasets used for jais-13b-chat which included a wide range of instructional data across various domains. We cover a wide range of common tasks including question answering, code generation, and reasoning over textual content. To enhance performance in Arabic, we developed an in-house Arabic dataset as well as translating some open-source English instructions into Arabic.\n\nThe pretraining data has a cutoff of December 2022, with some tuning data being more recent, up to October 2023.",
    "summary": "JAIS 30b Chat is an auto-regressive bilingual LLM for Arabic & English with state-of-the-art capabilities in Arabic.",
    "model_family": "Core42",
    "model_version": "1",
    "notes": "## Contact model provider\n\nTo submit any feedback [go to the original model card](https://huggingface.co/core42/jais-30b-chat-v3) and open an issue.\n\n## Responsible AI Considerations\n\nThe model is trained on publicly available data which was in part curated by Inception. We have employed different techniques to reduce bias in the model. While efforts have been made to minimize biases, it is likely that the model, as with all LLM models, will exhibit some bias.\n\nThe model is trained as an AI assistant for Arabic and English speakers. The model is limited to producing responses for queries in these two languages and may not produce appropriate responses to other language queries.\n\nBy using JAIS, you acknowledge and accept that, as with any large language model, it may generate incorrect, misleading and/or offensive information or content. The content generated by JAIS is not intended as advice and should not be relied upon in any way, nor are we responsible for any of the content or consequences resulting from its use. We are continuously working to develop models with greater capabilities, and as such, welcome any feedback on the model.\n",
    "tags": [
      "conversation",
      "multilingual",
      "rag"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "ar"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 8192,
    "training_data_date": "Dec 2022",
    "evaluation": "# Evaluation\n\n_NOTE: The evaluation report below was provided by the model publisher._\n\nCore42 conducted a comprehensive evaluation of Jais-30b-chat and benchmarked it against other leading base and instruction finetuned language models, focusing on both English and Arabic. Benchmarks used have a significant overlap with the widely used [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) tasks. The evaluation criteria span various dimensions, including:\n\n- Knowledge: How well the model answers factual questions.\n- Reasoning: The model's ability to answer questions that require reasoning.\n- Misinformation/Bias: Assessment of the model's susceptibility to generating false or misleading information, and its neutrality.  \n\nThe following results report F1 or Accuracy (depending on the task) of the evaluated models on benchmarked tasks. Both metrics are higher the better.  \n\n## Arabic Benchmark Results\n\n| Models              | Avg    | EXAMS | MMLU (M) | LitQA | Hellaswag | PIQA | BoolQA | SituatedQA | ARC-C | OpenBookQA | TruthfulQA | CrowS-Pairs | \n|---------------------|--------|-------|----------|-------|-----------|------|--------|------------|-------|------------|------------|-------------|\n| Jais-30b-chat | **51.3** | **40.7** | **35.1** | **57.1** | 59.3 | 64.1 | **81.6** | **52.9** | 39.1 | 29.6 | **53.1** | 52.5 |\n| Jais-chat (13B) | 49.22 | 39.7 | 34  | 52.6 | **61.4** | **67.5** | 65.7 | 47  | **40.7** | 31.6 | 44.8 | **56.4** |\n| acegpt-13b-chat | 45.94 | 38.6 | 31.2 | 42.3 | 49.2 | 60.2 | 69.7 | 39.5 | 35.1 | **35.4** | 48.2 | 55.9 |\n| BLOOMz (7.1B) | 43.65 | 34.9 | 31  | 44  | 38.1 | 59.1 | 66.6 | 42.8 | 30.2 | 29.2 | 48.4 | 55.8 |\n| acegpt-7b-chat | 43.36 | 37  | 29.6 | 39.4 | 46.1 | 58.9 | 55  | 38.8 | 33.1 | 34.6 | 50.1 | 54.4 |\n| aya-101-13b-chat | 41.92 | 29.9 | 32.5 | 38.3 | 35.6 | 55.7 | 76.2 | 42.2 | 28.3 | 29.4 | 42.8 | 50.2 |\n| mT0-XXL (13B) | 41.41 | 31.5 | 31.2 | 36.6 | 33.9 | 56.1 | 77.8 | 44.7 | 26.1 | 27.8 | 44.5 | 45.3 |\n| LLama2-70b-chat | 39.4 | 29.7 | 29.3 | 33.7 | 34.3 | 52  | 67.3 | 36.4 | 26.4 | 28.4 | 46.3 | 49.6 |\n| Llama2-13b-chat | 38.73 | 26.3 | 29.1 | 33.1 | 32  | 52.1 | 66  | 36.3 | 24.1 | 28.4 | 48.6 | 50  |\n\nFor evaluations, the focus is on LLMs that are multilingual or Arabic centric, except for Llama2 13B-chat and Llama2-70B-chat models. Among Arabic centric models like AceGPT and multilingual models like Aya, both Jais models outperform all other models by 4+ points. Jais models outperforming English only LLMs such as Llama2-13B-chat and LLama2-70B-chat demonstrates the obvious - though these models are trained on more tokens (2T) and in one case is much larger, Jais\u2019 Arabic centric training gives it a dramatic advantage in Arabic linguistic tasks. Note that LLama\u2019s pretraining may include traces of Arabic as evidenced by its limited yet observable capability to understand Arabic, but it is insufficient to obtain an LLM capable of conversing in Arabic, as is expected.\n\n## English Benchmark Results\n\n| Models              | Avg      | MMLU | RACE | Hellaswag | PIQA | BoolQA | SituatedQA | ARC-C | OpenBookQA | Winogrande | TruthfulQA | CrowS-Pairs | \n|---------------------|----------|------|------|-----------|------|--------|------------|-------|------------|------------|------------|-------------|\n| Jais-30b-chat | 59.59 | 36.5 | 45.6 | 78.9 | 73.1 | 90  | 56.7 | **51.2** | **44.4** | 70.2 | 42.3 | 66.6 |\n| Jais-chat (13B) | 57.45 | 37.7 | 40.8 | 77.6 | 78.2 | 75.8 | 57.8 | 46.8 | 41  | 68.6 | 39.7 | 68  |\n| acegpt-13b-chat | 57.84 | 34.4 | 42.7 | 76  | 78.8 | 81.9 | 45.4 | 45  | 41.6 | 71.3 | 45.7 | **73.4** |\n| BLOOMz (7.1B) | 57.81 | 36.7 | 45.6 | 63.1 | 77.4 | **91.7** | **59.7** | 43.6 | 42  | 65.3 | 45.2 | 65.6 |\n| acegpt-7b-chat | 54.25 | 30.9 | 40.1 | 67.6 | 75.4 | 75.3 | 44.2 | 38.8 | 39.6 | 66.3 | 49.3 | 69.3 |\n| aya-101-13b-chat | 49.55 | 36.6 | 41.3 | 46  | 65.9 | 81.9 | 53.5 | 31.2 | 33  | 56.2 | 42.5 | 57  |\n| mT0-XXL (13B) | 50.21 | 34  | 43.6 | 42.2 | 67.6 | 87.6 | 55.4 | 29.4 | 35.2 | 54.9 | 43.4 | 59  |\n| LLama2-70b-chat | **61.25** | **43** | 45.2 | **80.3** | **80.6** | 86.5 | 46.5 | 49  | 43.8 | **74** | **52.8** | 72.1 |\n| Llama2-13b-chat | 58.05 | 36.9 | **45.7** | 77.6 | 78.8 | 83  | 47.4 | 46  | 42.4 | 71  | 44.1 | 65.7 |\n\nJais-30b-chat outperforms the best other multilingual/ Arabic centric model in English language capabilities by ~2 points. Note that the best model among other Arabic centric models is AceGPT, which finetunes from Llama2-13B. Llama2 models (13B and 70B) are both pre-trained on far more English tokens (2T) vs those that were used for the pretrained Jais-30b (0.97T). At less than half the model and pretraining data size, Jais models reach within 2 points of the English capabilities of Llama2-70B-chat.\n\n## Cultural/ Local Context Knowledge\n\nOne of the key motivations to train an Arabic LLM is to include knowledge specific to the local context. In training Jais-30b-chat, we have invested considerable effort to include data that reflects high quality knowledge in both languages in the UAE and regional domains. To evaluate the impact of this training, in addition to LM harness evaluations in the general language domain, we also evaluate Jais models on a dataset testing knowledge pertaining to the UAE/regional domain. We curated ~320 UAE + Region specific factual questions in both English and Arabic. Each question has four answer choices, and like in the LM Harness, the task for the LLM is to choose the correct one. The following table shows Accuracy for both Arabic and English subsets of this test set.\n\n| Model | Arabic | English |\n| --- | --- | --- |\n| Jais-30b-chat | 57.2 | 55  |\n\n## Long Context Evaluations\n\nWe adopted the needle-in-haystack approach to assess the model's capability of handling long contexts. In this\nevaluation setup, we input a lengthy irrelevant text (the haystack) along with a required fact to answer a question (the\nneedle), which is embedded within this text. The model's task is to answer the question by locating and extracting the\nneedle from the text.\n\nWe plot the accuracies of the model at retrieving the needle from the given context. We conducted evaluations for both Arabic and English languages. For brevity, we are presenting the plot for Arabic only.\n\nWe observe that jais-30b-chat-v3 is improved over jais-30b-chat-v1 as it can answer the question upto 8k context\nlengths.\n\n<p align=\"center\">  <img src=\"https://huggingface.co/core42/jais-30b-chat-v3/resolve/main/jais-30b-chat-v1-needle.png\" width=\"600\" /></p>\n<p align=\"center\">  <img src=\"https://huggingface.co/core42/jais-30b-chat-v3/resolve/main/jais-30b-chat-v3-needle.png\" width=\"600\" /></p>\n",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/jais-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-core42/jais-30b-chat",
    "page": 1
  },
  {
    "id": "Ministral-3B",
    "registry": "azureml-mistral",
    "name": "Ministral 3B",
    "original_name": "Ministral-3B",
    "friendly_name": "Ministral 3B",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "**Ministral 3B** is a state-of-the-art Small Language Model (SLM) optimized for edge computing and on-device applications. As it is designed for low-latency and compute-efficient inference, it it also the perfect model for standard GenAI applications that have real-time requirements and high-volume.\n\n**Number of Parameters:** 3,6 billions\n\nMinistral 3B and Ministral 8B set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.\n\n## Use cases\n\nOur most innovative customers and partners have increasingly been asking for local, privacy-first inference for critical applications such as on-device translation, internet-less smart assistants, local analytics, and autonomous robotics. Les Ministraux were built to provide a compute-efficient and low-latency solution for these scenarios. From independent hobbyists to global manufacturing teams, les Ministraux deliver for a wide variety of use cases.\n\nUsed in conjunction with larger language models such as Mistral Large, les Ministraux are also efficient intermediaries for function-calling in multi-step agentic workflows. They can be tuned to handle input parsing, task routing, and calling APIs based on user intent across multiple contexts at extremely low latency and cost.\n\n_Source: [Un Ministral, des Ministraux - Introducing the world\u2019s best edge models.](https://mistral.ai/news/ministraux/)_\n",
    "summary": "Ministral 3B is a state-of-the-art Small Language Model (SLM) optimized for edge computing and on-device applications. As it is designed for low-latency and compute-efficient inference, it it also the perfect model for standard GenAI applications that have",
    "model_family": "Mistral AI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "low latency",
      "agents",
      "reasoning"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "fr",
      "de",
      "es",
      "it",
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "_Source: [Un Ministral, des Ministraux - Introducing the world\u2019s best edge models.](https://mistral.ai/news/ministraux/)_\n\nWe demonstrate the performance of les Ministraux across multiple tasks where they consistently outperform their peers. We re-evaluated all models with our internal framework for fair comparison.\n\n## Pretrained Models\n\n### Knowledge & Commonsense\n\n| Model        | MMLU     | AGIEval  | Winogrande | Arc-c    | TriviaQA |\n|:-------------|:---------|:---------|:-----------|:---------|:---------|\n| Gemma 2 2B   | 52.4     | 33.8     | 68.7       | 42.6     | 47.8     |\n| Llama 3.2 3B | 56.2     | 37.4     | 59.6       | 43.1     | 50.7     |\n| Ministral 3B | **60.9** | **42.1** | **72.7**   | **64.2** | **56.7** |\n| Mistral 7B   | 62.4     | 42.5     | 74.2       | 67.9     | 62.5     |\n| Llama 3.1 8B | 64.7     | 44.4     | 74.6       | 46.0     | 60.2     |\n| Ministral 8B | **65.0** | **48.3** | **75.3**   | **71.9** | **65.5** |\n\n### Code and Math\n\n| Model        | HumanEval (pass@1) | GSM8K (maj@8) |\n|:-------------|:-------------------|:--------------|\n| Gemma 2 2B   | 20.1               | 35.5          |\n| Llama 3.2 3B | 29.9               | 37.2          |\n| Ministral 3B | **34.2**           | **50.9**      |\n| Mistral 7B   | 26.8               | 51.3          |\n| Llama 3.1 8B | **37.8**           | 61.7          |\n| Ministral 8B | 34.8               | **64.5**      |\n\n### Multilingual\n\n| Model        | French MMLU | German MMLU | Spanish MMLU |\n|:-------------|:------------|:------------|:-------------|\n| Gemma 2 2B   | 41.0        | 40.1        | 41.7         |\n| Llama 3.2 3B | 42.3        | 42.2        | 43.1         |\n| Ministral 3B | **49.1**    | **48.3**    | **49.5**     |\n| Mistral 7B   | 50.6        | 49.6        | 51.4         |\n| Llama 3.1 8B | **50.8**    | 52.8        | 54.6         |\n| Ministral 8B | 57.5        | **57.4**    | **59.6**     |\n\n## Instruct Models\n\n### Chat/Arena (gpt-4o judge)\n\n| Model        | MTBench | Arena Hard | Wild bench |\n|:-------------|:--------|:-----------|:-----------|\n| Gemma 2 2B   | 7.5     | 51.7       | 32.5       |\n| Llama 3.2 3B | 7.2     | 46.0       | 27.2       |\n| Ministral 3B | **8.1** | **64.3**   | **36.3**   |\n| Mistral 7B   | 6.7     | 44.3       | 33.1       |\n| Llama 3.1 8B | 7.5     | 62.4       | 37.0       |\n| Gemma 2 9B   | 7.6     | 68.7       | **43.8**   |\n| Ministral 8B | **8.3** | **70.9**   | 41.3       |\n\n### Code and Math\n\n| Model        | MBPP (pass@1) | HumanEval (pass@1) | Math (maj@1) |\n|:-------------|:--------------|:-------------------|:-------------|\n| Gemma 2 2B   | 54.5          | 42.7               | 22.8         |\n| Llama 3.2 3B | 64.6          | 61.0               | 38.4         |\n| Ministral 3B | **67.7**      | **77.4**           | **51.7**     |\n| Mistral 7B   | 50.2          | 38.4               | 13.2         |\n| Llama 3.1 8B | 69.7          | 67.1               | 49.3         |\n| Gemma 2 9B   | 68.5          | 67.7               | 47.4         |\n| Ministral 8B | **70.0**      | **76.8**           | **54.5**     |\n",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Ministral-3B",
    "page": 1
  },
  {
    "id": "Llama-3.2-90B-Vision-Instruct",
    "registry": "azureml-meta",
    "name": "Llama-3.2-90B-Vision-Instruct",
    "original_name": "Llama-3.2-90B-Vision-Instruct",
    "friendly_name": "Llama-3.2-90B-Vision-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\n**Model Developer**: Meta\n\n## Model Architecture\n\nLlama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Data volume | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2-Vision  | (Image, text) pairs | 11B (10.6) | Text \\+ Image | Text  | 128k\\* | Yes | 6B (image, text) pairs | December 2023 |\n| Llama 3.2-Vision | (Image, text) pairs | 90B (88.8) | Text \\+ Image | Text  | 128k\\* | Yes | 6B (image, text) pairs  | December 2023 |\n\n\\* Note: Serverless APIs on Azure AI currently only support 8K context length.\n\n**Supported Languages:** For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported. \n\nDevelopers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n## Training Data\n\n**Overview:** Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n",
    "summary": "Advanced image reasoning capabilities for visual understanding agentic apps.",
    "model_family": "Meta",
    "model_version": "1",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2). For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n## License\n\nUse of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\nThis is a Llama 3.2 multimodal modal. Under the License and AUP, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not granted to any individual domiciled in, or any company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any multimodal models.\n\n## Intended Uses\n\n**Intended Use Cases:** Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Vision\u2019s ability to take images and text as inputs, additional use cases could include:\n\n1. Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.   \n2. Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.  \n3. Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.  \n4. Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.  \n5. Visual Grounding: Visual grounding is like connecting the dots between what we see and say. It\u2019s about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.  \n   \n**Built with Llama**\n\nThe Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases. \n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n3. Provide protections for the community to help prevent the misuse of our models.\n\n### Responsible Deployment \n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n#### Llama 3.2 Instruct \n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well., \n\n**Image Reasoning:** Llama 3.2-Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. \n\n**Red teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks \n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2-Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.  \n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2\u2019s vision capabilities are not generally germane to cyber uplift, we believe that the testing conducted for Llama 3.1 also applies to Llama 3.2.\n\n### Community \n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\n**Testing:** But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.   \n",
    "tags": [
      "multimodal",
      "reasoning",
      "conversation"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 128000,
    "training_data_date": "",
    "evaluation": "In this section, we report the results for Llama 3.2-Vision models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 11B | Llama 3.2 90B |\n| ----- | ----- | ----- | ----- | ----- | ----- |\n| Image Understanding | VQAv2 (val) | 0 | Accuracy | 66.8 | 73.6 |\n|  | Text VQA (val) | 0 | Relaxed accuracy | 73.1 | 73.5 |\n|  | DocVQA (val, unseen) | 0 | ANLS | 62.3 | 70.7 |\n| Visual Reasoning | MMMU (val, 0-shot) | 0 | Micro average accuracy | 41.7 | 49.3 |\n|  | ChartQA (test) | 0 | Accuracy | 39.4 | 54.2 |\n|  | InfographicsQA (val, unseen) | 0 | ANLS | 43.2 | 56.8 |\n|  | AI2 Diagram (test) | 0 | Accuracy | 62.4 | 75.3 |\n\n### Instruction Tuned Models\n\n| Modality | Capability | Benchmark | \\# Shots | Metric | Llama 3.2 11B | Llama 3.2 90B |\n| ----- | :---: | ----- | :---: | :---: | ----- | ----- |\n| Image | College-level Problems and Mathematical Reasoning | MMMU (val, CoT) | 0 | Micro average accuracy | 50.7 | 60.3 |\n|  |  | MMMU-Pro, Standard (10 opts, test) | 0 | Accuracy | 33.0 | 45.2 |\n|  |  | MMMU-Pro, Vision (test) | 0 | Accuracy | 23.7 | 33.8 |\n|  |  | MathVista (testmini) | 0 | Accuracy | 51.5 | 57.3 |\n|  | Charts and Diagram Understanding | ChartQA (test, CoT) | 0 | Relaxed accuracy | 83.4 | 85.5 |\n|  |  | AI2 Diagram (test) | 0 | Accuracy | 91.1 | 92.3 |\n|  |  | DocVQA (test) | 0 | ANLS | 88.4 | 90.1 |\n|  | General Visual Question Answering | VQAv2 (test) | 0 | Accuracy | 75.2 | 78.1 |\n|  |  |  |  |  |  |  |\n| Text | General | MMLU (CoT) | 0 | Macro\\_avg/acc | 73.0 | 86.0 |\n|  | Math | MATH (CoT) | 0 | Final\\_em | 51.9 | 68.0 |\n|  | Reasoning | GPQA | 0 | Accuracy | 32.8 | 46.7 |\n|  | Multilingual | MGSM (CoT) | 0 | em | 68.9 | 86.9 |\n",
    "license_description": "Use of Llama 3.2 is governed by the Llama 3.2 Community License <https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE> (a custom, commercial license agreement).",
    "static_model": false,
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Llama-3-2-90B-Vision-Instruct",
    "page": 1
  },
  {
    "id": "Llama-3.2-11B-Vision-Instruct",
    "registry": "azureml-meta",
    "name": "Llama-3.2-11B-Vision-Instruct",
    "original_name": "Llama-3.2-11B-Vision-Instruct",
    "friendly_name": "Llama-3.2-11B-Vision-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\n**Model Developer**: Meta\n\n## Model Architecture\n\nLlama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Data volume | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2-Vision  | (Image, text) pairs | 11B (10.6) | Text \\+ Image | Text  | 128k | Yes | 6B (image, text) pairs | December 2023 |\n| Llama 3.2-Vision | (Image, text) pairs | 90B (88.8) | Text \\+ Image | Text  | 128k | Yes | 6B (image, text) pairs  | December 2023 |\n\n**Supported Languages:** For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported. \n\nDevelopers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n## Training Data\n\n**Overview:** Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n",
    "summary": "Excels in image reasoning capabilities on high-res images for visual understanding apps.",
    "model_family": "Meta",
    "model_version": "1",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2). For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n## License\n\nUse of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\nThis is a Llama 3.2 multimodal modal. Under the License and AUP, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not granted to any individual domiciled in, or any company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any multimodal models.\n\n## Intended Uses\n\n**Intended Use Cases:** Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Vision\u2019s ability to take images and text as inputs, additional use cases could include:\n\n1. Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.   \n2. Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.  \n3. Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.  \n4. Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.  \n5. Visual Grounding: Visual grounding is like connecting the dots between what we see and say. It\u2019s about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.  \n   \n**Built with Llama**\n\nThe Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases. \n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n3. Provide protections for the community to help prevent the misuse of our models.\n\n### Responsible Deployment \n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n#### Llama 3.2 Instruct \n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well., \n\n**Image Reasoning:** Llama 3.2-Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. \n\n**Red teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks \n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2-Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.  \n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2\u2019s vision capabilities are not generally germane to cyber uplift, we believe that the testing conducted for Llama 3.1 also applies to Llama 3.2.\n\n### Community \n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\n**Testing:** But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.   \n",
    "tags": [
      "multimodal",
      "reasoning",
      "conversation"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 128000,
    "training_data_date": "Dec 2023",
    "evaluation": "The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\n**Model Developer**: Meta\n\n## Model Architecture\n\nLlama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Data volume | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2-Vision  | (Image, text) pairs | 11B (10.6) | Text \\+ Image | Text  | 128k | Yes | 6B (image, text) pairs | December 2023 |\n| Llama 3.2-Vision | (Image, text) pairs | 90B (88.8) | Text \\+ Image | Text  | 128k | Yes | 6B (image, text) pairs  | December 2023 |\n\n**Supported Languages:** For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported. \n\nDevelopers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n## Training Data\n\n**Overview:** Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n",
    "license_description": "Use of Llama 3.2 is governed by the Llama 3.2 Community License <https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE> (a custom, commercial license agreement).",
    "static_model": false,
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Llama-3-2-11B-Vision-Instruct",
    "page": 1
  },
  {
    "id": "Cohere-command-r-plus-08-2024",
    "registry": "azureml-cohere",
    "name": "Cohere Command R+ 08-2024",
    "original_name": "Cohere-command-r-plus-08-2024",
    "friendly_name": "Cohere Command R+ 08-2024",
    "task": "chat-completion",
    "publisher": "Cohere",
    "license": "custom",
    "description": "Command R+ 08-2024 is a highly performant generative large language model, optimized for a variety of use cases including reasoning, summarization, and question answering. \n\nThe model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n\nPre-training data additionally included the following 13 languages: Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian.\n\n## Model Architecture\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.\n\n### Tool use capabilities\nCommand R+ 08-2024 has been specifically trained with conversational tool use capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation.\n\nCommand R+\u2019s tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R+ may use one of its supplied tools more than once.\n\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn\u2019t want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\n\n### Grounded Generation and RAG Capabilities\n\nCommand R+ 08-2024 has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG).This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\n\nCommand R+\u2019s grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\n\nBy default, Command R+ will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\n\nThe model is trained with a number of other answering modes, which can be selected by prompt changes . A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\n\n### Code Capabilities\nCommand R+ 08-2024 has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\n\n### Structured Outputs\nStructured Outputs ensures outputs from Cohere\u2019s Command R+ 08-2024 model adheres to a user-defined response format. It supports JSON response format, including user-defined JSON schemas. This enables developers to reliably and consistently generate model outputs for programmatic usage and reliable function calls. Some examples include extracting data, formulating queries, and displaying model outputs in the UI.",
    "summary": "Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads.",
    "model_family": "Cohere",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety]('https://learn.microsoft.com/azure/ai-services/content-safety/overview'). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more.]('https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview')\n",
    "tags": [
      "rag",
      "multilingual"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "it",
      "de",
      "pt-br",
      "ja",
      "ko",
      "zh-cn",
      "ar"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/cohere-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-cohere/Cohere-command-r-plus-08-2024",
    "page": 1
  },
  {
    "id": "Cohere-command-r-08-2024",
    "registry": "azureml-cohere",
    "name": "Cohere Command R 08-2024",
    "original_name": "Cohere-command-r-08-2024",
    "friendly_name": "Cohere Command R 08-2024",
    "task": "chat-completion",
    "publisher": "Cohere",
    "license": "custom",
    "description": "Command R 08-2024  is a highly performant generative large language model, optimized for a variety of use cases including reasoning, summarization, and question answering. \n\nThe model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n\nPre-training data additionally included the following 13 languages: Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian.\n\n## Model Architecture\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.\n\n### Tool use capabilities\nCommand R 08-2024 has been specifically trained with conversational tool use capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation.\n\nCommand R\u2019s tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R may use one of its supplied tools more than once.\n\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn\u2019t want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\n\n### Grounded Generation and RAG Capabilities\n\nCommand R 08-2024 has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG).This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\n\nCommand R\u2019s grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\n\nBy default, Command R will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\n\nThe model is trained with a number of other answering modes, which can be selected by prompt changes . A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\n\n### Code Capabilities\nCommand R 08-2024 has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\n\n### Structured Outputs\nStructured Outputs ensures outputs from Cohere\u2019s Command R 08-2024 model adheres to a user-defined response format. It supports JSON response format, including user-defined JSON schemas. This enables developers to reliably and consistently generate model outputs for programmatic usage and reliable function calls. Some examples include extracting data, formulating queries, and displaying model outputs in the UI.",
    "summary": "Command R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise.",
    "model_family": "Cohere",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety]('https://learn.microsoft.com/azure/ai-services/content-safety/overview'). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more.]('https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview')",
    "tags": [
      "rag",
      "multilingual"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "it",
      "de",
      "pt-br",
      "ja",
      "ko",
      "zh-cn",
      "ar"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/cohere-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-cohere/Cohere-command-r-08-2024",
    "page": 1
  },
  {
    "id": "Phi-3.5-MoE-instruct",
    "registry": "azureml",
    "name": "Phi-3.5-MoE instruct (128k)",
    "original_name": "Phi-3.5-MoE-instruct",
    "friendly_name": "Phi-3.5-MoE instruct (128k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n### Resources\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n### Model Architecture\nPhi-3.5-MoE has 16x3.8B parameters with 6.6B active parameters when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.\n\n### Training Data\nThis is a static model trained on an offline dataset with 4.9T tokens and a cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.\n",
    "summary": "A new mixture of experts model",
    "model_family": "Microsoft",
    "model_version": "5",
    "notes": "## Intended Use\n\n### Primary Use Cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n### Out-of-Scope Use Cases\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n* Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n* Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n* Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n* Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n* Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n* Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n* Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:  \n* Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n* High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n* Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n* Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n* Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training Data\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n\n",
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "ar",
      "zh",
      "cs",
      "da",
      "nl",
      "fi",
      "fr",
      "de",
      "he",
      "hu",
      "it",
      "ja",
      "ko",
      "no",
      "pl",
      "pt",
      "ru",
      "es",
      "sv",
      "th",
      "tr",
      "uk"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Aug 2024",
    "evaluation": "To understand the capabilities, we compare Phi-3.5-MoE with a set of models over a variety of benchmarks using our internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\n\n| Category | Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | Arena Hard | 37.9 | 39.4 | 25.7 | 42.0 | 55.2 | 75.0 |\n| | BigBench Hard CoT (0-shot) | 79.1 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n| | MMLU (5-shot) | 78.9 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n| | MMLU-Pro (0-shot, CoT) | 54.3 | 40.7 | 44.0 | 50.1 | 57.2 | 62.8 |\n| Reasoning | ARC Challenge (10-shot) | 91.0 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n| | BoolQ (2-shot) | 84.6 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n| | GPQA (0-shot, CoT) | 36.8 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n| | HellaSwag (5-shot) | 83.8 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n| | OpenBookQA (10-shot) | 89.6 | 84.4 | 84.8 | 89.6 | 89.0 | 90.0 |\n| | PIQA (5-shot) | 88.6 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n| | Social IQA (5-shot) | 78.0 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n| | TruthfulQA (MC2) (10-shot) | 77.5 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n| | WinoGrande (5-shot) | 81.3 | 70.4 | 64.7 | 74.0 | 74.7 | 76.9 |\n| Multilingual | Multilingual MMLU (5-shot) | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| | MGSM (0-shot CoT) | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| Math | GSM8K (8-shot, CoT) | 88.7 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n| | MATH (0-shot, CoT) | 59.5 | 31.2 | 47.6 | 50.9 | 38.0 | 70.2 |\n| Long context | Qasper | 40.0 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n| | SQuALITY | 24.1 | 25.8 | 26.2 | 0.0 | 23.5 | 23.8 |\n| Code Generation | HumanEval (0-shot) | 70.7 | 63.4 | 66.5 | 61.0 | 74.4 | 86.6 |\n| | MBPP (3-shot) | 80.8 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **69.2** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | 62.6 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning | 78.7 | 72.2 | 70.5 | 75.4 | 77.7 | 80.0 |\n| Language understanding | 71.8 | 67.0 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness | 75.6 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context | 25.5 | 24.5 | 25.5 | 0.0 | 27.0 | 25.4 |\n| Math | 74.1 | 57.7 | 65.0 | 67.9 | 60.2 | 80.8 |\n| Code generation | 68.3 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual | 65.8 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, Phi-3.5-MoE with only **6.6B active parameters** achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4o-mini. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.\n\n### Multilingual\n\nThe table below highlights multilingual capability of Phi-3.5-MoE on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 6.6B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\n\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Multilingual MMLU | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| Multilingual MMLU-Pro | 45.3 | 34.0 | 21.4 | 43.0 | 57.9 | 53.2 |\n| MGSM | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| MEGA MLQA | 65.3 | 61.2 | 45.2 | 54.4 | 61.6 | 70.0 |\n| MEGA TyDi QA | 67.1 | 63.7 | 54.5 | 65.6 | 63.6 | 81.8 |\n| MEGA UDPOS | 60.4 | 58.2 | 54.1 | 56.6 | 62.4 | 66.0 |\n| MEGA XCOPA | 76.6 | 10.8 | 21.1 | 31.2 | 95.0 | 90.3 |\n| MEGA XStoryCloze | 82.8 | 92.3 | 71.0 | 87.0 | 20.7 | 96.6 |\n| **Average** | **65.8** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\n### Long Context\n\nPhi-3.5-MoE supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, multilingual context retrieval. We see that Phi-3.5 is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-MoE-instruct is very competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|\n| GovReport | 26.4 | 25.6 | 25.1 | 27.8 | 24.8 |\n| QMSum | 19.9 | 22.1 | 21.6 | 24.0 | 21.7 |\n| Qasper | 40.0 | 30.7 | 37.2 | 43.5 | 39.8 |\n| SQuALITY | 24.1 | 25.8 | 26.2 | 23.5 | 23.8 |\n| SummScreenFD | 16.9 | 18.2 | 17.6 | 16.3 | 17.0 |\n| **Average** | **25.5** | **24.5** | **25.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 94.8 | 93 | 93.2 | 91.6 | 85.7 | 64.2 | **87.1** |\n| Llama-3.1-8B-instruct | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| Mistral-Nemo-12B-instruct-2407 | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 89 | 74 | 81 | 88 | 95 | **85** |\n| Llama-3.1-8B-instruct | 80 | 65 | 73 | 76 | 63 | **71** |\n| Mistral-7B-instruct-v0.3 | 61 | 57 | 51 | 61 | 80 | **62** |\n",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-5-MoE-instruct",
    "page": 1
  },
  {
    "id": "o1-preview",
    "registry": "azure-openai",
    "name": "OpenAI o1-preview",
    "original_name": "o1-preview",
    "friendly_name": "OpenAI o1-preview",
    "task": "chat-completion",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "#### OpenAI's o1 Series Models: Enhanced Reasoning and Problem Solving on Azure\n\nThe OpenAI o1 series models are specifically designed to tackle reasoning and problem-solving tasks with increased focus and capability. These models spend more time processing and understanding the user's request, making them exceptionally strong in areas like science, coding, math and similar fields. For example, o1 can be used by healthcare researchers to annotate cell sequencing data, by physicists to generate complicated mathematical formulas needed for quantum optics, and by developers in all fields to build and execute multi-step workflows.\n\nNote: Configurable content filters are currently not available for o1-preview and o1-mini.\n\n_IMPORTANT: o1-preview model is available for limited access. To try the model in the playground, registration is required, and access will be granted based on Microsoft\u2019s eligibility criteria._\n\n## Key Capabilities of the o1 Series\n\n- Complex Code Generation: Capable of generating algorithms and handling advanced coding tasks to support developers.\n- Advanced Problem Solving: Ideal for comprehensive brainstorming sessions and addressing multifaceted challenges.\n- Complex Document Comparison: Perfect for analyzing contracts, case files, or legal documents to identify subtle differences.\n- Instruction Following and Workflow Management: Particularly effective for managing workflows requiring shorter contexts.\n\n## Model Variants\n\n- o1-preview: The most capable model in the o1 series, offering enhanced reasoning abilities.\n- o1-mini: A faster and more cost-efficient option in the o1 series, ideal for coding tasks requiring speed and lower resource consumption.\n\n## Limitations\n\no1-preview model is currently in preview and do not include some features available in other models, such as image understanding and structured outputs found in the GPT-4o and GPT-4o-mini models. For many tasks, the generally available GPT-4o models may still be more suitable.\n\n## Resources\n\n- [OpenaI o1-mini model announcement](https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/)\n- [OpenAI o1-preview model announcement](https://openai.com/index/introducing-openai-o1-preview/)\n- [Azure OpenAI blog announcement](https://aka.ms/new-models)\n",
    "summary": "Focused on advanced reasoning and solving complex problems, including math and science tasks. Ideal for applications that require deep contextual understanding and agentic workflows.",
    "model_family": "OpenAI",
    "model_version": "1",
    "notes": "## Model provider\n\nThis model is provided through the Azure OpenAI Service.\n\n## Relevant documents\n\nThe following documents are applicable:\n\n- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/legal/cognitive-services/openai/overview)\n- [Transparency Note for Azure OpenAI Service](https://learn.microsoft.com/legal/cognitive-services/openai/transparency-note)\n\n## Safety\n\nOpenAI has incorporated additional safety measures into the o1 models, including new techniques to help the models refuse unsafe requests. These advancements make the o1 series some of the most robust models available.\n\nOpenAI measures safety is by testing how well models continue to follow its safety rules if a user tries to bypass them (known as \"jailbreaking\"). In OpenAI\u2019s internal tests, GPT-4o scored 22 (on a scale of 0-100) while o1-preview model scored 84. You can read more about this in the OpenAI\u2019s [system card](https://openai.com/index/openai-o1-system-card/) and [research post](https://openai.com/index/learning-to-reason-with-llms/).\n",
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "rate_limit_tier": "custom",
    "supported_languages": [
      "en",
      "it",
      "af",
      "es",
      "de",
      "fr",
      "id",
      "ru",
      "pl",
      "uk",
      "el",
      "lv",
      "zh",
      "ar",
      "tr",
      "ja",
      "sw",
      "cy",
      "ko",
      "is",
      "bn",
      "ur",
      "ne",
      "th",
      "pa",
      "mr",
      "te"
    ],
    "max_output_tokens": 32768,
    "max_input_tokens": 128000,
    "training_data_date": "Oct 2023",
    "evaluation": "_The following page is an extract from [Learning to Reason with LLMs, OpenAI blog, Sept 2024](https://openai.com/index/learning-to-reason-with-llms/). Please refer to the original source for a full benchmark report._\n\nOpenAI o1 ranks in the 89th percentile on competitive programming questions (Codeforces), places among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME), and exceeds human PhD-level accuracy on a benchmark of physics, biology, and chemistry problems (GPQA). While the work needed to make this new model as easy to use as current models is still ongoing, we are releasing an early version of this model, OpenAI o1-preview, for immediate use in ChatGPT and to trusted API users(opens in a new window).\n\nOur large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.\n\n## Evals\n\n| Dataset                          | Metric     | gpt-4o | o1-preview |\n|----------------------------------|------------|--------|------------|\n| Competition Math<br/>AIME (2024) | cons@64    | 13.4   | 56.7       |\n|                                  | pass@1     | 9.3    | 44.6       |\n| Competition Code<br/>CodeForces  | Elo        | 808    | 1,258      |\n|                                  | Percentile | 11.0   | 62.0       |\n| GPQA Diamond                     | cons@64    | 56.1   | 78.3       |\n|                                  | pass@1     | 50.6   | 73.3       |\n| Biology                          | cons@64    | 63.2   | 73.7       |\n|                                  | pass@1     | 61.6   | 65.9       |\n| Chemistry                        | cons@64    | 43.0   | 60.2       |\n|                                  | pass@1     | 40.2   | 59.9       |\n| Physics                          | cons@64    | 68.6   | 89.5       |\n|                                  | pass@1     | 59.5   | 89.4       |\n| MATH                             | pass@1     | 60.3   | 85.5       |\n| MMLU                             | pass@1     | 88.0   | 92.3       |\n| MMMU (val)                       | pass@1     | 69.1   | n/a        |\n| MathVista (testmini)             | pass@1     | 63.8   | n/a        |\n\n## Safety\n\n| Metric                                                                                                                                                   | GPT-4o | o1-preview |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------|--------|------------|\n| **% Safe completions on harmful prompts Standard**                                                                                                       | 0.990  | 0.995      |\n| **% Safe completions on harmful prompts Challenging: jailbreaks & edge cases**                                                                           | 0.714  | 0.934      |\n| \u21b3 Harassment (severe)                                                                                                                                    | 0.845  | 0.900      |\n| \u21b3 Exploitative sexual content                                                                                                                            | 0.483  | 0.949      |\n| \u21b3 Sexual content involving minors                                                                                                                        | 0.707  | 0.931      |\n| \u21b3 Advice about non-violent wrongdoing                                                                                                                    | 0.688  | 0.961      |\n| \u21b3 Advice about violent wrongdoing                                                                                                                        | 0.778  | 0.963      |\n| **% Safe completions for top 200 with highest Moderation API scores per category in WildChat**<br/>[Zhao, et al. 2024](https://arxiv.org/abs/2405.01470) | 0.945  | 0.971      |\n| **Goodness@0.1 StrongREJECT jailbreak eval**<br/>[Souly et al. 2024](https://arxiv.org/abs/2402.10260)                                                   | 0.220  | 0.840      |\n| **Human sourced jailbreak eval**                                                                                                                         | 0.770  | 0.960      |\n| **% Compliance on internal benign edge cases \u201cnot over-refusal\u201d**                                                                                        | 0.910  | 0.930      |\n| **% Compliance on benign edge cases in XSTest**<br/>[R\u00f6ttger, et al. 2023](https://arxiv.org/abs/2308.01263)                                             | 0.924  | 0.976      |\n",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/o1-preview",
    "page": 1
  },
  {
    "id": "o1-mini",
    "registry": "azure-openai",
    "name": "OpenAI o1-mini",
    "original_name": "o1-mini",
    "friendly_name": "OpenAI o1-mini",
    "task": "chat-completion",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "#### OpenAI's o1 Series Models: Enhanced Reasoning and Problem Solving on Azure\n\nThe OpenAI o1 series models are specifically designed to tackle reasoning and problem-solving tasks with increased focus and capability. These models spend more time processing and understanding the user's request, making them exceptionally strong in areas like science, coding, math and similar fields. For example, o1 can be used by healthcare researchers to annotate cell sequencing data, by physicists to generate complicated mathematical formulas needed for quantum optics, and by developers in all fields to build and execute multi-step workflows.\n\no1-mini is developed to provide a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.\n\nNote: Configurable content filters are currently not available for o1-preview and o1-mini.\n\n_IMPORTANT: o1-mini model is available for limited access. To try the model in the playground, registration is required, and access will be granted based on Microsoft\u2019s eligibility criteria._\n\n## Key Capabilities of the o1 Series\n\n- Complex Code Generation: Capable of generating algorithms and handling advanced coding tasks to support developers.\n- Advanced Problem Solving: Ideal for comprehensive brainstorming sessions and addressing multifaceted challenges.\n- Complex Document Comparison: Perfect for analyzing contracts, case files, or legal documents to identify subtle differences.\n- Instruction Following and Workflow Management: Particularly effective for managing workflows requiring shorter contexts.\n\n## Model Variants\n\n- o1-preview: The most capable model in the o1 series, offering enhanced reasoning abilities.\n- o1-mini: A faster and more cost-efficient option in the o1 series, ideal for coding tasks requiring speed and lower resource consumption.\n\n## Limitations\n\no1-mini model is currently in preview and do not include some features available in other models, such as image understanding and structured outputs found in the GPT-4o and GPT-4o-mini models. For many tasks, the generally available GPT-4o models may still be more suitable.\n\n## Resources\n\n- [OpenaI o1-mini model announcement](https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/)\n- [OpenAI o1-preview model announcement](https://openai.com/index/introducing-openai-o1-preview/)\n- [Azure OpenAI blog announcement](https://aka.ms/new-models)\n",
    "summary": "Smaller, faster, and 80% cheaper than o1-preview, performs well at code generation and small context operations.",
    "model_family": "OpenAI",
    "model_version": "1",
    "notes": "## Model provider\n\nThis model is provided through the Azure OpenAI Service.\n\n## Relevant documents\n\nThe following documents are applicable:\n\n- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/legal/cognitive-services/openai/overview)\n- [Transparency Note for Azure OpenAI Service](https://learn.microsoft.com/legal/cognitive-services/openai/transparency-note)\n\n## Safety\n\nOpenAI has incorporated additional safety measures into the o1 models, including new techniques to help the models refuse unsafe requests. These advancements make the o1 series some of the most robust models available.\n\nOpenAI measures safety is by testing how well models continue to follow its safety rules if a user tries to bypass them (known as \"jailbreaking\"). In OpenAI\u2019s internal tests, GPT-4o scored 22 (on a scale of 0-100) while o1-preview model scored 84. You can read more about this in the OpenAI\u2019s [system card](https://openai.com/index/openai-o1-system-card/) and [research post](https://openai.com/index/learning-to-reason-with-llms/).\n",
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "rate_limit_tier": "custom",
    "supported_languages": [
      "en",
      "it",
      "af",
      "es",
      "de",
      "fr",
      "id",
      "ru",
      "pl",
      "uk",
      "el",
      "lv",
      "zh",
      "ar",
      "tr",
      "ja",
      "sw",
      "cy",
      "ko",
      "is",
      "bn",
      "ur",
      "ne",
      "th",
      "pa",
      "mr",
      "te"
    ],
    "max_output_tokens": 65536,
    "max_input_tokens": 128000,
    "training_data_date": "Oct 2023",
    "evaluation": "_The following page is an extract from [OpenaI o1-mini model announcement](https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/). Please refer to the original source for a full benchmark report._\n\nLarge language models such as o1 are pre-trained on vast text datasets. While these high-capacity models have broad world knowledge, they can be expensive and slow for real-world applications. In contrast, o1-mini is a smaller model optimized for STEM reasoning during pretraining. After training with the same high-compute reinforcement learning (RL) pipeline as o1, o1-mini achieves comparable performance on many useful reasoning tasks, while being significantly more cost efficient.\n\n## Evals\n\n| Task   | Dataset                    | Metric             | GPT-4o | o1-mini | o1-preview |\n|--------|----------------------------|--------------------|--------|---------|------------|\n| Coding | Codeforces                 | Elo                | 900    | 1650    | 1258       |\n|        | HumanEval                  | Accuracy           | 90.2%  | 92.4%   | 92.4%      |\n|        | Cybersecurity CTFs         | Accuracy (Pass@12) | 20.0%  | 28.7%   | 43.0%      |\n| STEM   | MMLU (o-shot CoT)          |                    | 88.7%  | 85.2%   | 90.8%      |\n|        | GPQA (Diamond, 0-shot CoT) |                    | 53.6%  | 60.0%   | 73.3%      |\n|        | MATH-500 (0-shot CoT)      |                    | 60.3%  | 90.0%   | 858.5%     |\n\n## Safety\n\n| Metric                                                                                                 | GPT-4o | o1-mini |\n|--------------------------------------------------------------------------------------------------------|--------|---------|\n| **% Safe completions refusal on harmful prompts**<br/>(standard)                                       | 0.99   | 0.99    |\n| **% Safe completions on harmful prompts**<br/>(Challenging: jailbreaks & edge cases)                   | 0.714  | 0.932   |\n| **% Compliance on benign edge cases**<br/>(\u201cnot over-refusal\u201d)                                         | 0.91   | 0.923   |\n| **Goodness@0.1 StrongREJECT jailbreak eval**<br/>[Souly et al. 2024](https://arxiv.org/abs/2402.10260) | 0.22   | 0.83    |\n| **Human sourced jailbreak eval**                                                                       | 0.77   | 0.95    |\n",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/o1-mini",
    "page": 1
  },
  {
    "id": "Phi-3.5-vision-instruct",
    "registry": "azureml",
    "name": "Phi-3.5-vision instruct (128k)",
    "original_name": "Phi-3.5-vision-instruct",
    "friendly_name": "Phi-3.5-vision instruct (128k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n### Resources\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n### Model Summary\n|      |      |\n|------|------|\n| **Architecture** | Phi-3.5-vision has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model. |\n| **Inputs** | Text and Image. It\u2019s best suited for prompts using the chat format. |\n| **Context length** | 128K tokens |\n| **GPUs** | 256 A100-80G |\n| **Training time** | 6 days |\n| **Training data** | 500B tokens (vision tokens + text tokens) |\n| **Outputs** | Generated text in response to the input |\n| **Dates** | Trained between July and August 2024 |\n| **Status** | This is a static model trained on an offline text dataset with cutoff date March 15, 2024. Future versions of the tuned models may be released as we improve models. |\n| **Release date** | August 20, 2024 |\n| **License** | MIT |\n",
    "summary": "Refresh of Phi-3-vision model.",
    "model_family": "Microsoft",
    "model_version": "2",
    "notes": "## Intended Use\n\n### Primary Use Cases\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) General image understanding\n4) Optical character recognition\n5) Chart and table understanding\n6) Multiple image comparison\n7) Multi-image or video clip summarization\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Out-of-Scope Use Cases\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.** \n\n## Responsible AI Considerations\n\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n* Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\n* Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n* Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\n* Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\n* Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n* Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n* High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n* Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n* Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n* Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n* Identification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing).\n\n## Training Data\n\nOur training data includes a wide variety of sources, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;\n2) selected high-quality image-text interleave data;\n3) newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides, newly created multi-image and video data, e.g., short video clips/pair of two similar images;\n4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\n\n## Sample Inputs and Outputs (for real-time inference)\n\n### Sample Input\n\n```json\n{\n  \"input_data\": {\n    \"input_string\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            }\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n            }\n          },\n          {\n            \"type\": \"text\",\n            \"text\": \"What are in these images? What is the difference between two images?\"\n          }\n        ]\n      }\n    ],\n    \"parameters\": { \"temperature\": 0.7, \"max_new_tokens\": 2048 }\n  }\n}\n```\n\n### Sample Output\n\n```json\n{\n  \"output\": \" The first image depicts a serene, natural landscape featuring a boardwalk winding through a marsh-like area with tall grasses and a clear sky. The second image shows an urban setting with a stop sign in the foreground, a black SUV parked on the street, and traditional Chinese architecture in the background, including a red and gold gate with Chinese characters. The main difference is the setting: one is natural and tranquil, the other is urban and bustling.\"\n}\n```\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n",
    "tags": [
      "multimodal",
      "reasoning",
      "low latency"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Aug 2024",
    "evaluation": "In this release, the model enables multi-frame image understanding and reasoning which is based on valuable customer feedback. The hero example multi-frame capabilities include detailed image comparison, multi-image summarization/storytelling and video summarization, which have broad applications in many scenarios. We also observed performance improvement on most single image benchmarks, e.g., boosting MMMU performance from 40.2 to 43.0, MMBench performance from 80.5 to 81.9, document understanding benchmark TextVQA from 70.9 to 72.0. We believe most use cases will benefit from this release, but we encourage users to test the new model in their AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family and continue to welcome all the feedback from the community.\n\nBelow are the comparison results on existing multi-image benchmarks. On average, our model outperforms competitor models on the same size and competitive with much bigger models on multi-frame capabilities and video summarization.\n\nBLINK: a benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs.\n\n| Benchmark | Phi-3.5-vision-instrust | LlaVA-Interleave-Qwen-7B | InternVL-2-4B | InternVL-2-8B | Gemini-1.5-Flash | GPT-4o-mini | Claude-3.5-Sonnet | Gemini-1.5-Pro | GPT-4o |\n|--|--|--|--|--|--|--|--|--|--|\n| Art Style | 87.2 | 62.4 | 55.6 | 52.1 | 64.1 | 70.1 | 59.8 | 70.9 | 73.3 |\n| Counting | 54.2 | 56.7 | 54.2 | 66.7 | 51.7 | 55.0 | 59.2 | 65.0 | 65.0 |\n| Forensic Detection | 92.4 | 31.1 | 40.9 | 34.1 | 54.5 | 38.6 | 67.4 | 60.6 | 75.8 |\n| Functional Correspondence | 29.2 | 34.6 | 24.6 | 24.6 | 33.1 | 26.9 | 33.8 | 31.5 | 43.8 |\n| IQ Test | 25.3 | 26.7 | 26.0 | 30.7 | 25.3 | 29.3 | 26.0 | 34.0 | 19.3 |\n| Jigsaw | 68.0 | 86.0 | 55.3 | 52.7 | 71.3 | 72.7 | 57.3 | 68.0 | 67.3 |\n| Multi-View Reasoning | 54.1 | 44.4 | 48.9 | 42.9 | 48.9 | 48.1 | 55.6 | 49.6 | 46.6 |\n| Object Localization | 49.2 | 54.9 | 53.3 | 54.1 | 57.3 | 57.4 | 62.3 | 65.6 | 68.0 |\n| Relative Depth | 69.4 | 77.4 | 63.7 | 67.7 | 32.8 | 58.1 | 71.8 | 76.6 | 71.0 |\n| Relative Reflectance | 37.3 | 34.3 | 32.8 | 38.8 | 32.8 | 27.6 | 36.6 | 38.8 | 40.3 |\n| Semantic Correspondence | 36.7 | 31.7 | 31.7 | 22.3 | 32.4 | 31.7 | 45.3 | 48.9 | 54.0 |\n| Spatial Relation | 65.7 | 75.5 | 78.3 | 78.3 | 55.9 | 81.1 | 60.1 | 79.0 | 84.6 |\n| Visual Correspondence | 53.5 | 40.7 | 34.9 | 33.1 | 29.7 | 52.9 | 72.1 | 81.4 | 86.0 |\n| Visual Similarity | 83.0 | 91.9 | 48.1 | 45.2 | 47.4 | 77.8 | 84.4 | 81.5 | 88.1 |\n| **Overall** | **57.0** | **53.1** | **45.9** | **45.4** | **45.1** | **51.9** | **56.5** | **61.0** | **63.2** |\n\nVideo-MME: comprehensively assess the capabilities of MLLMs in processing video data, covering a wide range of visual domains, temporal durations, and data modalities.\n\n| Benchmark | Phi-3.5-vision-instrust | LlaVA-Interleave-Qwen-7B | InternVL-2-4B | InternVL-2-8B | Gemini-1.5-Flash | GPT-4o-mini | Claude-3.5-Sonnet | Gemini-1.5-Pro | GPT-4o |\n|--|--|--|--|--|--|--|--|--|--|\n| short (<2min) | 60.8 | 62.3 | 60.7 | 61.7 | 72.2 | 70.1 | 66.3 | 73.3 | 77.7 |\n| medium (4-15min) | 47.7 | 47.1 | 46.4 | 49.6 | 62.7 | 59.6 | 54.7 | 61.2 | 68.0 |\n| long (30-60) | 43.8 | 41.2 | 42.6 | 46.6 | 52.1 | 53.9 | 46.6 | 53.2 | 59.6 |\n| **Overall** | **50.8** | **50.2** | **49.9** | **52.6** | **62.3** | **61.2** | **55.9** | **62.6** | **68.4** |\n",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-5-vision-instruct",
    "page": 1
  },
  {
    "id": "AI21-Jamba-1.5-Mini",
    "registry": "azureml-ai21",
    "name": "AI21 Jamba 1.5 Mini",
    "original_name": "AI21-Jamba-1.5-Mini",
    "friendly_name": "AI21 Jamba 1.5 Mini",
    "task": "chat-completion",
    "publisher": "AI21 Labs",
    "license": "custom",
    "description": "Jamba 1.5 Mini is a state-of-the-art, hybrid SSM-Transformer instruction following foundation model. It's a Mixture-of-Expert model with 52B total parameters and 12B active parameters. The Jamba family of models are the most powerful & efficient long-context models on the market, offering a 256K context window, the longest available.. For long context input, they deliver up to 2.5X faster inference than leading models of comparable sizes. Jamba supports function calling/tool use, structured output (JSON), and grounded generation with citation mode and documents API. Jamba officially supports English, French, Spanish, Portuguese, German, Arabic and Hebrew, but can also work in many other languages.\n\n**Model Developer Name**: AI21 Labs\n\n## Model Architecture\nJamba 1.5 Mini is a state-of-the-art, hybrid SSM-Transformer instruction following foundation model                                                                                                                             \n## Model Variations\t\n52B total parameters and 12B active parameters\n\n## Model Input\t\nModel inputs text only.\n\n## Model Output\t\nModel generates text only.\n\n## Model Dates\nJamba 1.5 Mini was trained in Q3 2024 with data covering through early March 2024.\n\n## *\u200b\u200b\u200bModel Information Table*\n\n| **Name**             | **Params**         | **Content Length**  |\n|----------------------|--------------------|---------------------|\n| **Jamba 1.5 Mini**   | 52B (12B active)   | 256K                |\n| **Jamba 1.5 Large**  | 398B (94B active)  | 256K                |",
    "summary": "A 52B parameters (12B active) multilingual model, offering a 256K long context window, function calling, structured output, and grounded generation.",
    "model_family": "AI21 Labs",
    "model_version": "1",
    "notes": "## License\n\nPlease refer to [this link](https://www.ai21.com/licenses/jamba-open-model-license) for AI21's Terms of Use\n\nFor more information about how Azure processes data, for privacy and security in relation to Models-as-a-Service (MaaS), please see this Microsoft Docs [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-privacy?view=azureml-api-2#generate-inferencing-outputs-with-serverless-apis-models-as-a-service).\n\n## Ethical Considerations and Limitations\n\n### Ethical considerations\nAI21 Labs is on a mission to supercharge human productivity with machines working alongside humans as thought partners, thereby promoting human welfare and prosperity. To deliver its promise, this technology must be deployed and used in a responsible and sustainable way, taking into consideration potential risks, including malicious use by bad actors, accidental misuse and broader societal harms. We take these risks extremely seriously and put measures in place to mitigate them.\n\nAI21 provides open access to Jamba that can be used to power a large variety of useful applications. We believe it is important to ensure that this technology is used in a responsible way, while allowing developers the freedom they need to experiment rapidly and deploy solutions at scale. Overall, we view the safe implementation of this technology as a partnership and collaboration between AI21 and our customers and encourage engagement and dialogue to raise the bar on responsible usage.\n\nIn order to use Jamba, you are required to comply with our Terms of Use and with the following usage guidelines. Provided you comply with these requirements, you may use Jamba to power applications with live users without any additional approval. We reserve the right to limit or suspend your access to Jamba at any time where we believe these terms or guidelines are violated.\n\nPlease check these usage guidelines periodically, as they may be updated from time to time. For any questions, clarifications or concerns, please contact safety@ai21.com.\n\n### Limitations\nThere are a number of limitations inherent to neural networks technology that apply to Jamba. These limitations require explanation and carry important caveats for the application and usage of Jamba.\n\nAccuracy: Jamba, like other large pretrained language models, lacks important context about the world because it is trained on textual data and is not grounded in other modalities of experience such as video, real-world physical interaction, and human feedback. Like all language models, Jamba is far more accurate when responding to inputs similar to its training datasets. Novel inputs have a tendency to generate higher variance in its output.\n\nCoherence and consistency: Responses from Jamba are sometimes inconsistent, contradictory, or contain seemingly random sentences and paragraphs.\n\nWestern/English bias: Jamba is trained primarily on English language text from the internet, and is best suited to classifying, searching, summarizing, and generating English text. Furthermore, Jamba has a tendency to hold and amplify the biases contained in its training dataset. As a result, groups of people who were not involved in the creation of the training data can be underrepresented, and stereotypes and prejudices can be perpetuated. Racial, religious, gender, socioeconomic, and other categorizations of human groups can be considered among these factors.\n\nExplainability: It is difficult to explain or predict how Jamba will respond without additional training and fine tuning. This is a common issue with neural networks of this scope and scale.\n\nRecency: Jamba was trained on a dataset created in March 2024, and therefore has no knowledge of events that have occurred after that date. We update our models regularly to keep them as current as possible, but there are notable gaps and inaccuracies in responses as a result of this lack of recency.\n\n\n\n## Safety and Responsible Use\n\nAI21 Labs is on a mission to make AI-first experiences, with machines working alongside humans as thought partners, thereby promoting human welfare and prosperity. To deliver its promise, this technology must be deployed and used in a responsible and sustainable way, taking into consideration potential risks, including malicious use by bad actors, accidental misuse and broader societal harms. We take these risks extremely seriously and put measures in place to mitigate them.\n\nAI safety is an important challenge with a large surface area, which we believe can be addressed most effectively by working together. We invite anyone interested in conducting research or otherwise promoting AI safety to contact us at safety@ai21.com and explore opportunities for collaboration.\n\n## Content Filtering\n\nPrompts and completions in GitHub Models are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).\n\n",
    "tags": [
      "rag",
      "multilingual",
      "large context"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "pt",
      "de",
      "ar",
      "he"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 262144,
    "training_data_date": "",
    "evaluation": "### *Training Data*\n\nJamba is trained on an in-house dataset that contains text data from the web, books, and code. The knowledge cutoff date is March 5, 2024.\n\n#### *Evaluation Results*\n\n| **Category**          | **Metric**     | **Score** |\n|-----------------------|----------------|-----------|\n| General               | Arena Hard     | 46.1      |\n|                       | MMLU           | 69.7      |\n|                       | MMLU Pro (CoT) | 42.5      |\n|                       | IFEval         | 75.8      |\n|                       | BBH            | 53.4      |\n|                       | WildBench      | 42.4      |\n| Reasoning             | ARC-C          | 85.7      |\n|                       | GPQA           | 32.3      |\n| Math, Code & tool use | GSM8K          | 75.8      |\n|                       | HumanEval      | 62.8      |\n|                       | BFCL           | 80.6      |\n\n\n#### *Evaluation of pretrained LLMs on automatic safety benchmarks*\n\n|                      | **TruthfulQA**  |\n|----------------------|-----------------|\n| **Jamba 1.5 Mini**   | 54.1            |\n| **Jamba 1.5 Large**  | 58.3            |\n\n#### *Evaluation of fine-tuned LLMs on different safety datasets*\n\n|                      | **RealToxicity\\***  |\n|----------------------|---------------------|\n| **Jamba 1.5 Mini**   | 8.1                 |\n| **Jamba 1.5 Large**  | 6.7                 |\n\n\\* Lower score is better\n",
    "license_description": "This model is provided under the License Terms available at <https://www.ai21.com/licenses/jamba-open-model-license>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-ai21/AI21-Jamba-1-5-Mini",
    "page": 1
  },
  {
    "id": "AI21-Jamba-1.5-Large",
    "registry": "azureml-ai21",
    "name": "AI21 Jamba 1.5 Large",
    "original_name": "AI21-Jamba-1.5-Large",
    "friendly_name": "AI21 Jamba 1.5 Large",
    "task": "chat-completion",
    "publisher": "AI21 Labs",
    "license": "custom",
    "description": "Jamba 1.5 Large is a state-of-the-art, hybrid SSM-Transformer instruction following foundation model. It's a Mixture-of-Expert model with 94B total parameters and 398B active parameters. The Jamba family of models are the most powerful & efficient long-context models on the market, offering a 256K context window, the longest available.. For long context input, they deliver up to 2.5X faster inference than leading models of comparable sizes. Jamba supports function calling/tool use, structured output (JSON), and grounded generation with citation mode and documents API. Jamba officially supports English, French, Spanish, Portuguese, German, Arabic and Hebrew, but can also work in many other languages.\n\n**Model Developer Name**: Jamba 1.5 Large\n\n## Model Architecture\n\nJamba 1.5 Large is a state-of-the-art, hybrid SSM-Transformer instruction following foundation model\n\n## Model Variations\t \n\n94B total parameters and 398B active parameters\n\n## Model Input\n\t\nModels input text only.\n\n## Model Output\n\nModels generate text only.\n\n## Model Dates\n\nJamba 1.5 Large was trained in Q3 2024 with data covering through early March 2024.\n\n## Model Information Table\n\n| **Name**             | **Params**         | **Content Length**  |\n|----------------------|--------------------|---------------------|\n| **Jamba 1.5 Mini**   | 52B (12B active)   | 256K                |\n| **Jamba 1.5 Large**  | 398B (94B active)  | 256K                |",
    "summary": "A 398B parameters (94B active) multilingual model, offering a 256K long context window, function calling, structured output, and grounded generation.",
    "model_family": "AI21 Labs",
    "model_version": "1",
    "notes": "#### License\n\nPlease refer to [this link](https://www.ai21.com/licenses/jamba-open-model-license) for AI21's Terms of Use\n\nFor more information about how Azure processes data, for privacy and security in relation to Models-as-a-Service (MaaS), please see this Microsoft Docs [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-privacy?view=azureml-api-2#generate-inferencing-outputs-with-serverless-apis-models-as-a-service).\n\n#### Ethical Considerations and Limitations\n\nAI21 Labs is on a mission to make AI-first experiences, with machines working alongside humans as thought partners, thereby promoting human welfare and prosperity. To deliver its promise, this technology must be deployed and used in a responsible and sustainable way, taking into consideration potential risks, including malicious use by bad actors, accidental misuse and broader societal harms. We take these risks extremely seriously and put measures in place to mitigate them.\n\nAI safety is an important challenge with a large surface area, which we believe can be addressed most effectively by working together. We invite anyone interested in conducting research or otherwise promoting AI safety to contact us at safety@ai21.com and explore opportunities for collaboration.\n\n## Content Filtering\n\nPrompts and completions in GitHub Models are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).\n\n",
    "tags": [
      "rag",
      "multilingual",
      "large context"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "pt",
      "de",
      "ar",
      "he"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 262144,
    "training_data_date": "",
    "evaluation": "### Training Data\n\nJamba is trained on an in-house dataset that contains text data from the web, books, and code. The knowledge cutoff date is March 5, 2024.\n\n\n### Evaluation Results \n\n| **Category**          | **Metric**     | **Score** |\n|-----------------------|----------------|-----------|\n| General               | Arena Hard     | 65.4      |\n|                       | MMLU (CoT)     | 81.2      |\n|                       | MMLU Pro (CoT) | 53.5      |\n|                       | IFEval         | 81.5      |\n|                       | BBH            | 65.5      |\n|                       | WildBench      | 48.4      |\n| Reasoning             | ARC-C          | 93        |\n|                       | GPQA           | 36.9      |\n| Math, Code & Tool use | GSM8K          | 87        |\n|                       | HumanEval      | 71.3      |\n|                       | BFCL           | 85.5      |\n\n\n\n\n### Evaluation of pretrained LLMs on automatic safety benchmarks*\n\n|                      | **TruthfulQA**  |\n|----------------------|-----------------|\n| **Jamba 1.5 Mini**   | 54.1            |\n| **Jamba 1.5 Large**  | 58.3            |\n\n### Evaluation of fine-tuned LLMs on different safety datasets*\n\n|                      | **RealToxicity\\***  |\n|----------------------|---------------------|\n| **Jamba 1.5 Mini**   | 8.1                 |\n| **Jamba 1.5 Large**  | 6.7                 |\n\n\\* Lower score is better\n",
    "license_description": "This model is provided under the License Terms available at <https://www.ai21.com/licenses/jamba-open-model-license>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-ai21/AI21-Jamba-1-5-Large",
    "page": 1
  },
  {
    "id": "Phi-3.5-mini-instruct",
    "registry": "azureml",
    "name": "Phi-3.5-mini instruct (128k)",
    "original_name": "Phi-3.5-mini-instruct",
    "friendly_name": "Phi-3.5-mini instruct (128k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n### Resources\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n### Model Architecture\nPhi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini. It is a text-only model best suited for prompts using chat format.\n\n### Training Data\nPhi-3.5-mini is a static model trained on an offline dataset with 3.4T tokens and a cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.",
    "summary": "Refresh of Phi-3-mini model.",
    "model_family": "Microsoft",
    "model_version": "6",
    "notes": "## Intended Use\n\n### Primary Use Cases\nThe model is intended for broad commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Out-of-Scope Use Cases\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**\n\n## Responsible AI Considerations\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n* Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n* Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n* Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n* Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n* Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n* Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n* Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift.\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\n\n* Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n* High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n* Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n* Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n* Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training Data\nOur training data includes a wide variety of sources, totaling 3.4 trillion tokens, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n## Sample Inputs and Outputs (for real-time inference)\n\n### **Sample input**\n```json\n{\n  \"input_data\": {\n    \"input_string\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"I am going to Paris, give me a list of 10 places to visit\"\n      }\n    ],\n    \"parameters\": {\n      \"temperature\": 0.7,\n      \"top_p\": 0.9,\n      \"do_sample\": true,\n      \"max_new_tokens\": 1000\n    }\n  }\n}\n``` \n\n### **Sample output**\n```json\n{\n  \"output\": \" 1. Eiffel Tower: Visit the iconic symbol of Paris, offering breathtaking views of the city.\\n\\n2. Louvre Museum: Explore one of the world's largest and most visited museums, home to thousands of works of art, including the Mona Lisa.\\n\\n3. Notre-Dame Cathedral: Marvel at the stunning Gothic architecture of this famous cathedral, although note that it is currently under renovation due to the 2019 fire.\\n\\n4. Montmartre: Discover this historic and artistic neighborhood, famous for its bohemian past and the stunning Sacr\u00e9-C\u0153ur Basilica.\\n\\n5. Seine River Cruise: Take a relaxing cruise on the Seine River, seeing some of the city's most famous landmarks like the Louvre, Notre-Dame, and the Eiffel Tower from a unique perspective.\\n\\n6. Champs-\u00c9lys\u00e9es: Visit this famous avenue lined with shops, cafes, and theaters. Don't forget to check out the Arc de Triomphe at its end.\\n\\n7. Palace of Versailles: Take a day trip from Paris to explore the opulent palace and gardens of Versailles, a UNESCO World Heritage site.\\n\\n8. Sacr\u00e9-C\u0153ur Basilica: Located at the highest point in the city, this basilica offers panoramic views of Paris.\\n\\n9. Latin Quarter: Stroll through this historic and vibrant neighborhood, famous for its student life, lively atmosphere, and cafes.\\n\\n10. Mus\u00e9e d'Orsay: Visit this museum, housing an impressive collection of Impressionist and Post-Impressionist art, including works by Monet, Degas, Renoir, and Van Gogh.\"\n}\n```\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.",
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "ar",
      "zh",
      "cs",
      "da",
      "nl",
      "fi",
      "fr",
      "de",
      "he",
      "hu",
      "it",
      "ja",
      "ko",
      "no",
      "pl",
      "pt",
      "ru",
      "es",
      "sv",
      "th",
      "tr",
      "uk"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Aug 2024",
    "evaluation": "### Multilingual\nThe table below highlights multilingual capability of Phi-3.5-mini on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 3.8B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\n\n| Benchmark | Phi-3.5-mini-instruct | Phi-3.0-mini-128k-instruct | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|---|---|---|---|---|---|---|---|---|\n| Multilingual MMLU | 55.4 | 51.08 | 47.4 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| Multilingual MMLU-Pro | 30.9 | 30.21 | 15.0 | 34.0 | 21.4 | 43.0 | 57.9 | 53.2 |\n| MGSM | 47.9 | 41.56 | 31.8 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| MEGA MLQA | 61.7 | 55.5 | 43.9 | 61.2 | 45.2 | 54.4 | 61.6 | 70.0 |\n| MEGA TyDi QA | 62.2 | 55.9 | 54.0 | 63.7 | 54.5 | 65.6 | 63.6 | 81.8 |\n| MEGA UDPOS | 46.5 | 48.1 | 57.2 | 58.2 | 54.1 | 56.6 | 62.4 | 66.0 |\n| MEGA XCOPA | 63.1 | 62.4 | 58.8 | 10.8 | 21.1 | 31.2 | 95.0 | 90.3 |\n| MEGA XStoryCloze | 73.5 | 73.6 | 75.5 | 92.3 | 71.0 | 87.0 | 20.7 | 96.6 |\n| **Average** | **55.2** | **52.3** | **47.9** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\n### Long Context\n\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. Phi-3.5-mini outperforms Gemma-2 family which only supports 8K context length and is competitive with other much larger open-weight models such as Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, and Mistral-Nemo-12B-Instruct-2407.\n\n| Benchmark | Phi-3.5-mini-instruct | Llama-3.1-8B-instruct | Mistral-7B-instruct-v0.3 | Mistral-Nemo-12B-instruct-2407 | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|---|---|---|---|---|---|---|\n| GovReport | 25.9 | 25.1 | 26.0 | 25.6 | 27.8 | 24.8 |\n| QMSum | 21.3 | 21.6 | 21.3 | 22.1 | 24.0 | 21.7 |\n| Qasper | 41.9 | 37.2 | 31.4 | 30.7 | 43.5 | 39.8 |\n| SQuALITY | 25.3 | 26.2 | 25.9 | 25.8 | 23.5 | 23.8 |\n| SummScreenFD | 16.0 | 17.6 | 17.5 | 18.2 | 16.3 | 17.0 |\n| **Average** | **26.1** | **25.5** | **24.4** | **24.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 94.3 | 91.1 | 90.7 | 87.1 | 78.0 | 63.6 | **84.1** |\n| **Llama-3.1-8B-instruct** | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| **Mistral-Nemo-12B-instruct-2407** | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 86 | 67 | 73 | 77 | 82 | **77** |\n| **Llama-3.1-8B-instruct** | 80 | 65 | 73 | 76 | 63 | **71** |\n| **Mistral-7B-instruct-v0.3** | 61 | 57 | 51 | 61 | 80 | **62** |",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-5-mini-instruct",
    "page": 1
  },
  {
    "id": "Meta-Llama-3.1-8B-Instruct",
    "registry": "azureml-meta",
    "name": "Meta-Llama-3.1-8B-Instruct",
    "original_name": "Meta-Llama-3.1-8B-Instruct",
    "friendly_name": "Meta-Llama-3.1-8B-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned\ngenerative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on\ncommon industry benchmarks.\n\n## Model Architecture\n\nLlama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.",
    "summary": "The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
    "model_family": "Meta",
    "model_version": "4",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on\nthe model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and\nrecipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes)\n\n## License\n\nA custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\n\n## Intended Uses\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only\nmodels are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n\\*\\*Note:  Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n\n# Responsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety\nrisks:\n\n\u25cf Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n\n\u25cf Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n\n\u25cf Provide protections for the community to help prevent the misuse of our models.\n\n## Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n### Llama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to\nmitigate potential safety risks. We\u02bcve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n\n### Llama 3.1 systems\n\\*\\*Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. \\*\\*Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use:** Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n\n**Multilinguality:** \\*\\*Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n\n## Evaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and\nwe used the learnings to improve our benchmarks and safety tuning datasets.\n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how\nsuch models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\n\n\n## Critical and other risks\n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model\u02bcs capability to produce outputs that\ncould result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B\u02bcs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n## Community\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u02bcs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://www.facebook.com/login.php?next=https%3A%2F%2Fdevelopers.facebook.com%2Fllama_output_feedback) and [bug bounty program](https://bugbounty.meta.com/?utm_source=facebook.com&utm_medium=redirect) to continuously improve the Llama technology with the help of the community.\n\n\n# Ethical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1\u02bcs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n\n\n# Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "conversation"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Dec 2023",
    "evaluation": "In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\n\n\n### Base pretrained models\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong># Shots</strong>\n        </td>\n        <td>\n            <strong>Metric</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 8B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 8B</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 70B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 70B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 405B</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"7\\\">\n            General\n        </td>\n        <td>\n            MMLU\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc_char\n        </td>\n        <td>\n            66.7\n        </td>\n        <td>\n            66.7\n        </td>\n        <td>\n            79.5\n        </td>\n        <td>\n            79.3\n        </td>\n        <td>\n            85.2\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU PRO (CoT)\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc_char\n        </td>\n        <td>\n            36.2\n        </td>\n        <td>\n            37.1\n        </td>\n        <td>\n            55.0\n        </td>\n        <td>\n            53.8\n        </td>\n        <td>\n            61.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            AGIEval English\n        </td>\n        <td>\n            3-5\n        </td>\n        <td>\n            average/acc_char\n        </td>\n        <td>\n            47.1\n        </td>\n        <td>\n            47.8\n        </td>\n        <td>\n            63.0\n        </td>\n        <td>\n            64.6\n        </td>\n        <td>\n            71.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            CommonSenseQA\n        </td>\n        <td>\n            7\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            72.6\n        </td>\n        <td>\n            75.0\n        </td>\n        <td>\n            83.8\n        </td>\n        <td>\n            84.1\n        </td>\n        <td>\n            85.8\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Winogrande\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            60.5\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            83.3\n        </td>\n        <td>\n            86.7\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BIG-Bench Hard (CoT)\n        </td>\n        <td>\n            3\n        </td>\n        <td>\n            average/em\n        </td>\n        <td>\n            61.1\n        </td>\n        <td>\n            64.2\n        </td>\n        <td>\n            81.3\n        </td>\n        <td>\n            81.6\n        </td>\n        <td>\n            85.9\n        </td>\n    </tr>\n    <tr>\n        <td>\n            ARC-Challenge\n        </td>\n        <td>\n            25\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            79.4\n        </td>\n        <td>\n            79.7\n        </td>\n        <td>\n            93.1\n        </td>\n        <td>\n            92.9\n        </td>\n        <td>\n            96.1\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Knowledge Reasoning\n        </td>\n        <td>\n            TriviaQA-Wiki\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            78.5\n        </td>\n        <td>\n            77.6\n        </td>\n        <td>\n            89.7\n        </td>\n        <td>\n            89.8\n        </td>\n        <td>\n            91.8\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Reading Comprehension\n        </td>\n        <td>\n            SQuAD\n        </td>\n        <td>\n            1\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            76.4\n        </td>\n        <td>\n            77.0\n        </td>\n        <td>\n            85.6\n        </td>\n        <td>\n            81.8\n        </td>\n        <td>\n            89.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            QuAC (F1)\n        </td>\n        <td>\n            1\n        </td>\n        <td>\n            f1\n        </td>\n        <td>\n            44.4\n        </td>\n        <td>\n            44.9\n        </td>\n        <td>\n            51.1\n        </td>\n        <td>\n            51.1\n        </td>\n        <td>\n            53.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BoolQ\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            75.7\n        </td>\n        <td>\n            75.0\n        </td>\n        <td>\n            79.0\n        </td>\n        <td>\n            79.4\n        </td>\n        <td>\n            80.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            DROP (F1)\n        </td>\n        <td>\n            3\n        </td>\n        <td>\n            f1\n        </td>\n        <td>\n            58.4\n        </td>\n        <td>\n            59.5\n        </td>\n        <td>\n            79.7\n        </td>\n        <td>\n            79.6\n        </td>\n        <td>\n            84.8\n        </td>\n    </tr>\n</table>\n\n### Instruction tuned models\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong># Shots</strong>\n        </td>\n        <td>\n            <strong>Metric</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 8B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 8B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 70B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 70B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 405B Instruct</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"3\\\">\n            General\n        </td>\n        <td>\n            MMLU\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            68.5\n        </td>\n        <td>\n            69.4\n        </td>\n        <td>\n            82.0\n        </td>\n        <td>\n            83.6\n        </td>\n        <td>\n            87.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            65.3\n        </td>\n        <td>\n            73.0\n        </td>\n        <td>\n            80.9\n        </td>\n        <td>\n            86.0\n        </td>\n        <td>\n            88.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU PRO (COT)\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            micro_avg/acc_char\n        </td>\n        <td>\n            45.5\n        </td>\n        <td>\n            48.3\n        </td>\n        <td>\n            63.4\n        </td>\n        <td>\n            66.4\n        </td>\n        <td>\n            73.3\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"2\\\">\n            Reasoning\n        </td>\n        <td>\n            ARC-C\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            82.4\n        </td>\n        <td>\n            83.4\n        </td>\n        <td>\n            94.4\n        </td>\n        <td>\n            94.8\n        </td>\n        <td>\n            96.9\n        </td>\n    </tr>\n    <tr>\n        <td>\n            GPQA\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            34.6\n        </td>\n        <td>\n            30.4\n        </td>\n        <td>\n            39.5\n        </td>\n        <td>\n            41.7\n        </td>\n        <td>\n            50.7\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Code\n        </td>\n        <td>\n            HumanEval\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            60.4\n        </td>\n        <td>\n            72.6\n        </td>\n        <td>\n            81.7\n        </td>\n        <td>\n            80.5\n        </td>\n        <td>\n            89.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MBPP ++ base version\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            70.6\n        </td>\n        <td>\n            72.8\n        </td>\n        <td>\n            82.5\n        </td>\n        <td>\n            86.0\n        </td>\n        <td>\n            88.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multipl-E HumanEval\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            50.8\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            65.5\n        </td>\n        <td>\n            75.2\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multipl-E MBPP\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            52.4\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            62.0\n        </td>\n        <td>\n            65.7\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"2\\\">\n            Math\n        </td>\n        <td>\n            GSM-8k (CoT)\n        </td>\n        <td>\n            8\n        </td>\n        <td>\n            em_maj1@1\n        </td>\n        <td>\n            80.6\n        </td>\n        <td>\n            84.5\n        </td>\n        <td>\n            93.0\n        </td>\n        <td>\n            95.1\n        </td>\n        <td>\n            96.8\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MATH (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            final_em\n        </td>\n        <td>\n            29.1\n        </td>\n        <td>\n            51.9\n        </td>\n        <td>\n            51.0\n        </td>\n        <td>\n            68.0\n        </td>\n        <td>\n            73.8\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Tool Use\n        </td>\n        <td>\n            API-Bank\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            48.3\n        </td>\n        <td>\n            82.6\n        </td>\n        <td>\n            85.1\n        </td>\n        <td>\n            90.0\n        </td>\n        <td>\n            92.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BFCL\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            60.3\n        </td>\n        <td>\n            76.1\n        </td>\n        <td>\n            83.0\n        </td>\n        <td>\n            84.8\n        </td>\n        <td>\n            88.5\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Gorilla Benchmark API Bench\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            1.7\n        </td>\n        <td>\n            8.2\n        </td>\n        <td>\n            14.7\n        </td>\n        <td>\n            29.7\n        </td>\n        <td>\n            35.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Nexus (0-shot)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            18.1\n        </td>\n        <td>\n            38.5\n        </td>\n        <td>\n            47.8\n        </td>\n        <td>\n            56.7\n        </td>\n        <td>\n            58.7\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multilingual\n        </td>\n        <td>\n            Multilingual MGSM (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            68.9\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            86.9\n        </td>\n        <td>\n            91.6\n        </td>\n    </tr>\n</table>\n\n### Multilingual benchmarks\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong>Language</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 8B</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 70B</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 405B</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"7\\\">\n            General\n        </td>\n        <td rowspan=\\\"7\\\">\n            MMLU (5-shot, macro_avg/acc)\n        </td>\n        <td>\n            Portuguese\n        </td>\n        <td>\n            62.12\n        </td>\n        <td>\n            80.13\n        </td>\n        <td>\n            84.95\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Spanish\n        </td>\n        <td>\n            62.45\n        </td>\n        <td>\n            80.05\n        </td>\n        <td>\n            85.08\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Italian\n        </td>\n        <td>\n            61.63\n        </td>\n        <td>\n            80.4\n        </td>\n        <td>\n            85.04\n        </td>\n    </tr>\n    <tr>\n        <td>\n            German\n        </td>\n        <td>\n            60.59\n        </td>\n        <td>\n            79.27\n        </td>\n        <td>\n            84.36\n        </td>\n    </tr>\n    <tr>\n        <td>\n            French\n        </td>\n        <td>\n            62.34\n        </td>\n        <td>\n            79.82\n        </td>\n        <td>\n            84.66\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Hindi\n        </td>\n        <td>\n            50.88\n        </td>\n        <td>\n            74.52\n        </td>\n        <td>\n            80.31\n        </td>\n    </tr>\n        <tr>\n        <td>\n            Thai\n        </td>\n        <td>\n            50.32\n        </td>\n        <td>\n            72.95\n        </td>\n        <td>\n            78.21\n        </td>\n    </tr>\n</table>\n",
    "license_description": "A custom commercial license, the Llama 3.1 Community License, is available at <https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE>.\n",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Meta-Llama-3-1-8B-Instruct",
    "page": 2
  },
  {
    "id": "Meta-Llama-3.1-70B-Instruct",
    "registry": "azureml-meta",
    "name": "Meta-Llama-3.1-70B-Instruct",
    "original_name": "Meta-Llama-3.1-70B-Instruct",
    "friendly_name": "Meta-Llama-3.1-70B-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned\ngenerative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on\ncommon industry benchmarks.\n\n## Model Architecture\n\nLlama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.",
    "summary": "The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
    "model_family": "Meta",
    "model_version": "4",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on\nthe model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and\nrecipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes)\n\n## License\n\nA custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\n\n## Intended Uses\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only\nmodels are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n\\*\\*Note:  Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n\n# Responsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety\nrisks:\n\n\u25cf Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n\n\u25cf Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n\n\u25cf Provide protections for the community to help prevent the misuse of our models.\n\n## Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n### Llama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to\nmitigate potential safety risks. We\u02bcve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n\n### Llama 3.1 systems\n\\*\\*Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. \\*\\*Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use:** Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n\n**Multilinguality:** \\*\\*Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n\n## Evaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and\nwe used the learnings to improve our benchmarks and safety tuning datasets.\n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how\nsuch models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\n\n\n## Critical and other risks\n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model\u02bcs capability to produce outputs that\ncould result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B\u02bcs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n## Community\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u02bcs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://www.facebook.com/login.php?next=https%3A%2F%2Fdevelopers.facebook.com%2Fllama_output_feedback) and [bug bounty program](https://bugbounty.meta.com/?utm_source=facebook.com&utm_medium=redirect) to continuously improve the Llama technology with the help of the community.\n\n\n# Ethical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1\u02bcs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n\n\n# Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "conversation"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Dec 2023",
    "evaluation": "In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\n\n\n### Base pretrained models\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong># Shots</strong>\n        </td>\n        <td>\n            <strong>Metric</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 8B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 8B</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 70B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 70B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 405B</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"7\\\">\n            General\n        </td>\n        <td>\n            MMLU\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc_char\n        </td>\n        <td>\n            66.7\n        </td>\n        <td>\n            66.7\n        </td>\n        <td>\n            79.5\n        </td>\n        <td>\n            79.3\n        </td>\n        <td>\n            85.2\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU PRO (CoT)\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc_char\n        </td>\n        <td>\n            36.2\n        </td>\n        <td>\n            37.1\n        </td>\n        <td>\n            55.0\n        </td>\n        <td>\n            53.8\n        </td>\n        <td>\n            61.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            AGIEval English\n        </td>\n        <td>\n            3-5\n        </td>\n        <td>\n            average/acc_char\n        </td>\n        <td>\n            47.1\n        </td>\n        <td>\n            47.8\n        </td>\n        <td>\n            63.0\n        </td>\n        <td>\n            64.6\n        </td>\n        <td>\n            71.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            CommonSenseQA\n        </td>\n        <td>\n            7\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            72.6\n        </td>\n        <td>\n            75.0\n        </td>\n        <td>\n            83.8\n        </td>\n        <td>\n            84.1\n        </td>\n        <td>\n            85.8\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Winogrande\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            60.5\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            83.3\n        </td>\n        <td>\n            86.7\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BIG-Bench Hard (CoT)\n        </td>\n        <td>\n            3\n        </td>\n        <td>\n            average/em\n        </td>\n        <td>\n            61.1\n        </td>\n        <td>\n            64.2\n        </td>\n        <td>\n            81.3\n        </td>\n        <td>\n            81.6\n        </td>\n        <td>\n            85.9\n        </td>\n    </tr>\n    <tr>\n        <td>\n            ARC-Challenge\n        </td>\n        <td>\n            25\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            79.4\n        </td>\n        <td>\n            79.7\n        </td>\n        <td>\n            93.1\n        </td>\n        <td>\n            92.9\n        </td>\n        <td>\n            96.1\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Knowledge Reasoning\n        </td>\n        <td>\n            TriviaQA-Wiki\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            78.5\n        </td>\n        <td>\n            77.6\n        </td>\n        <td>\n            89.7\n        </td>\n        <td>\n            89.8\n        </td>\n        <td>\n            91.8\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Reading Comprehension\n        </td>\n        <td>\n            SQuAD\n        </td>\n        <td>\n            1\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            76.4\n        </td>\n        <td>\n            77.0\n        </td>\n        <td>\n            85.6\n        </td>\n        <td>\n            81.8\n        </td>\n        <td>\n            89.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            QuAC (F1)\n        </td>\n        <td>\n            1\n        </td>\n        <td>\n            f1\n        </td>\n        <td>\n            44.4\n        </td>\n        <td>\n            44.9\n        </td>\n        <td>\n            51.1\n        </td>\n        <td>\n            51.1\n        </td>\n        <td>\n            53.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BoolQ\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            75.7\n        </td>\n        <td>\n            75.0\n        </td>\n        <td>\n            79.0\n        </td>\n        <td>\n            79.4\n        </td>\n        <td>\n            80.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            DROP (F1)\n        </td>\n        <td>\n            3\n        </td>\n        <td>\n            f1\n        </td>\n        <td>\n            58.4\n        </td>\n        <td>\n            59.5\n        </td>\n        <td>\n            79.7\n        </td>\n        <td>\n            79.6\n        </td>\n        <td>\n            84.8\n        </td>\n    </tr>\n</table>\n\n### Instruction tuned models\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong># Shots</strong>\n        </td>\n        <td>\n            <strong>Metric</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 8B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 8B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 70B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 70B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 405B Instruct</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"3\\\">\n            General\n        </td>\n        <td>\n            MMLU\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            68.5\n        </td>\n        <td>\n            69.4\n        </td>\n        <td>\n            82.0\n        </td>\n        <td>\n            83.6\n        </td>\n        <td>\n            87.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            65.3\n        </td>\n        <td>\n            73.0\n        </td>\n        <td>\n            80.9\n        </td>\n        <td>\n            86.0\n        </td>\n        <td>\n            88.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU PRO (COT)\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            micro_avg/acc_char\n        </td>\n        <td>\n            45.5\n        </td>\n        <td>\n            48.3\n        </td>\n        <td>\n            63.4\n        </td>\n        <td>\n            66.4\n        </td>\n        <td>\n            73.3\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"2\\\">\n            Reasoning\n        </td>\n        <td>\n            ARC-C\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            82.4\n        </td>\n        <td>\n            83.4\n        </td>\n        <td>\n            94.4\n        </td>\n        <td>\n            94.8\n        </td>\n        <td>\n            96.9\n        </td>\n    </tr>\n    <tr>\n        <td>\n            GPQA\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            34.6\n        </td>\n        <td>\n            30.4\n        </td>\n        <td>\n            39.5\n        </td>\n        <td>\n            41.7\n        </td>\n        <td>\n            50.7\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Code\n        </td>\n        <td>\n            HumanEval\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            60.4\n        </td>\n        <td>\n            72.6\n        </td>\n        <td>\n            81.7\n        </td>\n        <td>\n            80.5\n        </td>\n        <td>\n            89.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MBPP ++ base version\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            70.6\n        </td>\n        <td>\n            72.8\n        </td>\n        <td>\n            82.5\n        </td>\n        <td>\n            86.0\n        </td>\n        <td>\n            88.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multipl-E HumanEval\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            50.8\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            65.5\n        </td>\n        <td>\n            75.2\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multipl-E MBPP\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            52.4\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            62.0\n        </td>\n        <td>\n            65.7\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"2\\\">\n            Math\n        </td>\n        <td>\n            GSM-8k (CoT)\n        </td>\n        <td>\n            8\n        </td>\n        <td>\n            em_maj1@1\n        </td>\n        <td>\n            80.6\n        </td>\n        <td>\n            84.5\n        </td>\n        <td>\n            93.0\n        </td>\n        <td>\n            95.1\n        </td>\n        <td>\n            96.8\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MATH (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            final_em\n        </td>\n        <td>\n            29.1\n        </td>\n        <td>\n            51.9\n        </td>\n        <td>\n            51.0\n        </td>\n        <td>\n            68.0\n        </td>\n        <td>\n            73.8\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Tool Use\n        </td>\n        <td>\n            API-Bank\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            48.3\n        </td>\n        <td>\n            82.6\n        </td>\n        <td>\n            85.1\n        </td>\n        <td>\n            90.0\n        </td>\n        <td>\n            92.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BFCL\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            60.3\n        </td>\n        <td>\n            76.1\n        </td>\n        <td>\n            83.0\n        </td>\n        <td>\n            84.8\n        </td>\n        <td>\n            88.5\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Gorilla Benchmark API Bench\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            1.7\n        </td>\n        <td>\n            8.2\n        </td>\n        <td>\n            14.7\n        </td>\n        <td>\n            29.7\n        </td>\n        <td>\n            35.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Nexus (0-shot)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            18.1\n        </td>\n        <td>\n            38.5\n        </td>\n        <td>\n            47.8\n        </td>\n        <td>\n            56.7\n        </td>\n        <td>\n            58.7\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multilingual\n        </td>\n        <td>\n            Multilingual MGSM (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            68.9\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            86.9\n        </td>\n        <td>\n            91.6\n        </td>\n    </tr>\n</table>\n\n### Multilingual benchmarks\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong>Language</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 8B</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 70B</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 405B</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"7\\\">\n            General\n        </td>\n        <td rowspan=\\\"7\\\">\n            MMLU (5-shot, macro_avg/acc)\n        </td>\n        <td>\n            Portuguese\n        </td>\n        <td>\n            62.12\n        </td>\n        <td>\n            80.13\n        </td>\n        <td>\n            84.95\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Spanish\n        </td>\n        <td>\n            62.45\n        </td>\n        <td>\n            80.05\n        </td>\n        <td>\n            85.08\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Italian\n        </td>\n        <td>\n            61.63\n        </td>\n        <td>\n            80.4\n        </td>\n        <td>\n            85.04\n        </td>\n    </tr>\n    <tr>\n        <td>\n            German\n        </td>\n        <td>\n            60.59\n        </td>\n        <td>\n            79.27\n        </td>\n        <td>\n            84.36\n        </td>\n    </tr>\n    <tr>\n        <td>\n            French\n        </td>\n        <td>\n            62.34\n        </td>\n        <td>\n            79.82\n        </td>\n        <td>\n            84.66\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Hindi\n        </td>\n        <td>\n            50.88\n        </td>\n        <td>\n            74.52\n        </td>\n        <td>\n            80.31\n        </td>\n    </tr>\n        <tr>\n        <td>\n            Thai\n        </td>\n        <td>\n            50.32\n        </td>\n        <td>\n            72.95\n        </td>\n        <td>\n            78.21\n        </td>\n    </tr>\n</table>\n",
    "license_description": "A custom commercial license, the Llama 3.1 Community License, is available at <https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE>.\n",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Meta-Llama-3-1-70B-Instruct",
    "page": 2
  },
  {
    "id": "Meta-Llama-3.1-405B-Instruct",
    "registry": "azureml-meta",
    "name": "Meta-Llama-3.1-405B-Instruct",
    "original_name": "Meta-Llama-3.1-405B-Instruct",
    "friendly_name": "Meta-Llama-3.1-405B-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned\ngenerative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on\ncommon industry benchmarks.\n\n## Model Architecture\n\nLlama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.",
    "summary": "The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
    "model_family": "Meta",
    "model_version": "1",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on\nthe model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and\nrecipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes)\n\n## License\n\nA custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\n\n## Intended Uses\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only\nmodels are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n\\*\\*Note:  Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n\n# Responsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety\nrisks:\n\n\u25cf Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n\n\u25cf Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n\n\u25cf Provide protections for the community to help prevent the misuse of our models.\n\n## Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n### Llama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to\nmitigate potential safety risks. We\u02bcve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n\n### Llama 3.1 systems\n\\*\\*Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. \\*\\*Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use:** Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n\n**Multilinguality:** \\*\\*Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n\n## Evaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and\nwe used the learnings to improve our benchmarks and safety tuning datasets.\n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how\nsuch models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\n\n\n## Critical and other risks\n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model\u02bcs capability to produce outputs that\ncould result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B\u02bcs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n## Community\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u02bcs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://www.facebook.com/login.php?next=https%3A%2F%2Fdevelopers.facebook.com%2Fllama_output_feedback) and [bug bounty program](https://bugbounty.meta.com/?utm_source=facebook.com&utm_medium=redirect) to continuously improve the Llama technology with the help of the community.\n\n\n# Ethical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1\u02bcs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n\n\n# Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "conversation"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Dec 2023",
    "evaluation": "In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\n\n\n### Base pretrained models\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong># Shots</strong>\n        </td>\n        <td>\n            <strong>Metric</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 8B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 8B</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 70B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 70B</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 405B</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"7\\\">\n            General\n        </td>\n        <td>\n            MMLU\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc_char\n        </td>\n        <td>\n            66.7\n        </td>\n        <td>\n            66.7\n        </td>\n        <td>\n            79.5\n        </td>\n        <td>\n            79.3\n        </td>\n        <td>\n            85.2\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU PRO (CoT)\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc_char\n        </td>\n        <td>\n            36.2\n        </td>\n        <td>\n            37.1\n        </td>\n        <td>\n            55.0\n        </td>\n        <td>\n            53.8\n        </td>\n        <td>\n            61.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            AGIEval English\n        </td>\n        <td>\n            3-5\n        </td>\n        <td>\n            average/acc_char\n        </td>\n        <td>\n            47.1\n        </td>\n        <td>\n            47.8\n        </td>\n        <td>\n            63.0\n        </td>\n        <td>\n            64.6\n        </td>\n        <td>\n            71.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            CommonSenseQA\n        </td>\n        <td>\n            7\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            72.6\n        </td>\n        <td>\n            75.0\n        </td>\n        <td>\n            83.8\n        </td>\n        <td>\n            84.1\n        </td>\n        <td>\n            85.8\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Winogrande\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            60.5\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            83.3\n        </td>\n        <td>\n            86.7\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BIG-Bench Hard (CoT)\n        </td>\n        <td>\n            3\n        </td>\n        <td>\n            average/em\n        </td>\n        <td>\n            61.1\n        </td>\n        <td>\n            64.2\n        </td>\n        <td>\n            81.3\n        </td>\n        <td>\n            81.6\n        </td>\n        <td>\n            85.9\n        </td>\n    </tr>\n    <tr>\n        <td>\n            ARC-Challenge\n        </td>\n        <td>\n            25\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            79.4\n        </td>\n        <td>\n            79.7\n        </td>\n        <td>\n            93.1\n        </td>\n        <td>\n            92.9\n        </td>\n        <td>\n            96.1\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Knowledge Reasoning\n        </td>\n        <td>\n            TriviaQA-Wiki\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            78.5\n        </td>\n        <td>\n            77.6\n        </td>\n        <td>\n            89.7\n        </td>\n        <td>\n            89.8\n        </td>\n        <td>\n            91.8\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Reading Comprehension\n        </td>\n        <td>\n            SQuAD\n        </td>\n        <td>\n            1\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            76.4\n        </td>\n        <td>\n            77.0\n        </td>\n        <td>\n            85.6\n        </td>\n        <td>\n            81.8\n        </td>\n        <td>\n            89.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            QuAC (F1)\n        </td>\n        <td>\n            1\n        </td>\n        <td>\n            f1\n        </td>\n        <td>\n            44.4\n        </td>\n        <td>\n            44.9\n        </td>\n        <td>\n            51.1\n        </td>\n        <td>\n            51.1\n        </td>\n        <td>\n            53.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BoolQ\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc_char\n        </td>\n        <td>\n            75.7\n        </td>\n        <td>\n            75.0\n        </td>\n        <td>\n            79.0\n        </td>\n        <td>\n            79.4\n        </td>\n        <td>\n            80.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            DROP (F1)\n        </td>\n        <td>\n            3\n        </td>\n        <td>\n            f1\n        </td>\n        <td>\n            58.4\n        </td>\n        <td>\n            59.5\n        </td>\n        <td>\n            79.7\n        </td>\n        <td>\n            79.6\n        </td>\n        <td>\n            84.8\n        </td>\n    </tr>\n</table>\n\n### Instruction tuned models\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong># Shots</strong>\n        </td>\n        <td>\n            <strong>Metric</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 8B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 8B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#808080\\\">\n            <strong>Llama 3 70B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 70B Instruct</strong>\n        </td>\n        <td style=\\\"background-color:#007BA7\\\">\n            <strong>Llama 3.1 405B Instruct</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"3\\\">\n            General\n        </td>\n        <td>\n            MMLU\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            68.5\n        </td>\n        <td>\n            69.4\n        </td>\n        <td>\n            82.0\n        </td>\n        <td>\n            83.6\n        </td>\n        <td>\n            87.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            65.3\n        </td>\n        <td>\n            73.0\n        </td>\n        <td>\n            80.9\n        </td>\n        <td>\n            86.0\n        </td>\n        <td>\n            88.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MMLU PRO (COT)\n        </td>\n        <td>\n            5\n        </td>\n        <td>\n            micro_avg/acc_char\n        </td>\n        <td>\n            45.5\n        </td>\n        <td>\n            48.3\n        </td>\n        <td>\n            63.4\n        </td>\n        <td>\n            66.4\n        </td>\n        <td>\n            73.3\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"2\\\">\n            Reasoning\n        </td>\n        <td>\n            ARC-C\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            82.4\n        </td>\n        <td>\n            83.4\n        </td>\n        <td>\n            94.4\n        </td>\n        <td>\n            94.8\n        </td>\n        <td>\n            96.9\n        </td>\n    </tr>\n    <tr>\n        <td>\n            GPQA\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            34.6\n        </td>\n        <td>\n            30.4\n        </td>\n        <td>\n            39.5\n        </td>\n        <td>\n            41.7\n        </td>\n        <td>\n            50.7\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Code\n        </td>\n        <td>\n            HumanEval\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            60.4\n        </td>\n        <td>\n            72.6\n        </td>\n        <td>\n            81.7\n        </td>\n        <td>\n            80.5\n        </td>\n        <td>\n            89.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MBPP ++ base version\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            70.6\n        </td>\n        <td>\n            72.8\n        </td>\n        <td>\n            82.5\n        </td>\n        <td>\n            86.0\n        </td>\n        <td>\n            88.6\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multipl-E HumanEval\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            50.8\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            65.5\n        </td>\n        <td>\n            75.2\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multipl-E MBPP\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            pass@1\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            52.4\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            62.0\n        </td>\n        <td>\n            65.7\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"2\\\">\n            Math\n        </td>\n        <td>\n            GSM-8k (CoT)\n        </td>\n        <td>\n            8\n        </td>\n        <td>\n            em_maj1@1\n        </td>\n        <td>\n            80.6\n        </td>\n        <td>\n            84.5\n        </td>\n        <td>\n            93.0\n        </td>\n        <td>\n            95.1\n        </td>\n        <td>\n            96.8\n        </td>\n    </tr>\n    <tr>\n        <td>\n            MATH (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            final_em\n        </td>\n        <td>\n            29.1\n        </td>\n        <td>\n            51.9\n        </td>\n        <td>\n            51.0\n        </td>\n        <td>\n            68.0\n        </td>\n        <td>\n            73.8\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"4\\\">\n            Tool Use\n        </td>\n        <td>\n            API-Bank\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            48.3\n        </td>\n        <td>\n            82.6\n        </td>\n        <td>\n            85.1\n        </td>\n        <td>\n            90.0\n        </td>\n        <td>\n            92.0\n        </td>\n    </tr>\n    <tr>\n        <td>\n            BFCL\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            60.3\n        </td>\n        <td>\n            76.1\n        </td>\n        <td>\n            83.0\n        </td>\n        <td>\n            84.8\n        </td>\n        <td>\n            88.5\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Gorilla Benchmark API Bench\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            acc\n        </td>\n        <td>\n            1.7\n        </td>\n        <td>\n            8.2\n        </td>\n        <td>\n            14.7\n        </td>\n        <td>\n            29.7\n        </td>\n        <td>\n            35.3\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Nexus (0-shot)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            macro_avg/acc\n        </td>\n        <td>\n            18.1\n        </td>\n        <td>\n            38.5\n        </td>\n        <td>\n            47.8\n        </td>\n        <td>\n            56.7\n        </td>\n        <td>\n            58.7\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Multilingual\n        </td>\n        <td>\n            Multilingual MGSM (CoT)\n        </td>\n        <td>\n            0\n        </td>\n        <td>\n            em\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            68.9\n        </td>\n        <td>\n            -\n        </td>\n        <td>\n            86.9\n        </td>\n        <td>\n            91.6\n        </td>\n    </tr>\n</table>\n\n### Multilingual benchmarks\n\n<table>\n    <tr>\n        <td>\n            <strong>Category</strong>\n        </td>\n        <td>\n            <strong>Benchmark</strong>\n        </td>\n        <td>\n            <strong>Language</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 8B</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 70B</strong>\n        </td>\n        <td>\n            <strong>Llama 3.1 405B</strong>\n        </td>\n    </tr>\n    <tr>\n        <td rowspan=\\\"7\\\">\n            General\n        </td>\n        <td rowspan=\\\"7\\\">\n            MMLU (5-shot, macro_avg/acc)\n        </td>\n        <td>\n            Portuguese\n        </td>\n        <td>\n            62.12\n        </td>\n        <td>\n            80.13\n        </td>\n        <td>\n            84.95\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Spanish\n        </td>\n        <td>\n            62.45\n        </td>\n        <td>\n            80.05\n        </td>\n        <td>\n            85.08\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Italian\n        </td>\n        <td>\n            61.63\n        </td>\n        <td>\n            80.4\n        </td>\n        <td>\n            85.04\n        </td>\n    </tr>\n    <tr>\n        <td>\n            German\n        </td>\n        <td>\n            60.59\n        </td>\n        <td>\n            79.27\n        </td>\n        <td>\n            84.36\n        </td>\n    </tr>\n    <tr>\n        <td>\n            French\n        </td>\n        <td>\n            62.34\n        </td>\n        <td>\n            79.82\n        </td>\n        <td>\n            84.66\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Hindi\n        </td>\n        <td>\n            50.88\n        </td>\n        <td>\n            74.52\n        </td>\n        <td>\n            80.31\n        </td>\n    </tr>\n        <tr>\n        <td>\n            Thai\n        </td>\n        <td>\n            50.32\n        </td>\n        <td>\n            72.95\n        </td>\n        <td>\n            78.21\n        </td>\n    </tr>\n</table>\n",
    "license_description": "A custom commercial license, the Llama 3.1 Community License, is available at <https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE>.\n",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Meta-Llama-3-1-405B-Instruct",
    "page": 2
  },
  {
    "id": "Mistral-large-2407",
    "registry": "azureml-mistral",
    "name": "Mistral Large (2407)",
    "original_name": "Mistral-large-2407",
    "friendly_name": "Mistral Large (2407)",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "Mistral Large (2407) is an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge and coding capabilities.\n\n**Multi-lingual by design.**\u00a0Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish\n\n**Proficient in coding.** Trained on 80+ coding languages such as Python, Java, C, C++, JavaScript, and Bash. Also trained on more specific languages such as Swift and Fortran\n\n**Agent-centric.** Best-in-class agentic capabilities with native function calling and JSON outputting \n\n**Advanced Reasoning.**\u00a0State-of-the-art mathematical and reasoning capabilities",
    "summary": "Mistral Large (2407) is an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge and coding capabilities.",
    "model_family": "Mistral AI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "rag",
      "agents"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "zh",
      "ja",
      "ko",
      "pt",
      "nl",
      "pl"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Mistral-large-2407",
    "page": 2
  },
  {
    "id": "Mistral-Nemo",
    "registry": "azureml-mistral",
    "name": "Mistral Nemo",
    "original_name": "Mistral-Nemo",
    "friendly_name": "Mistral Nemo",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "Mistral Nemo is a cutting-edge Language Model (LLM) boasting state-of-the-art reasoning, world knowledge, and coding capabilities within its size category.\n\n**Jointly developed with Nvidia.** This collaboration has resulted in a powerful 12B model that pushes the boundaries of language understanding and generation.\n\n**Multilingual proficiency.** Mistral Nemo is equipped with a new tokenizer, Tekken, designed for multilingual applications. It supports over 100 languages, including but not limited to English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, Polish, and many more. Tekken has proven to be more efficient than the Llama 3 tokenizer in compressing text for approximately 85% of all languages, with significant improvements in Malayalam, Hindi, Arabic, and prevalent European languages.\n\n**Agent-centric.** Mistral Nemo possesses top-tier agentic capabilities, including native function calling and JSON outputting.\n\n**Advanced Reasoning.** Mistral Nemo demonstrates state-of-the-art mathematical and reasoning capabilities within its size category.",
    "summary": "Mistral Nemo is a cutting-edge Language Model (LLM) boasting state-of-the-art reasoning, world knowledge, and coding capabilities within its size category.",
    "model_family": "Mistral AI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "rag",
      "agents"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "zh",
      "ja",
      "ko",
      "pt",
      "nl",
      "pl"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Mistral-Nemo",
    "page": 2
  },
  {
    "id": "Phi-3-small-8k-instruct",
    "registry": "azureml",
    "name": "Phi-3-small instruct (8k)",
    "original_name": "Phi-3-small-8k-instruct",
    "friendly_name": "Phi-3-small instruct (8k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "The Phi-3-Small-8K-Instruct is a 7B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model supports 8K context length (in tokens).\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Small-8K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\ud83d\udee0\ufe0f [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3 Small-8K-Instruct has 7B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).",
    "summary": "A 7B parameters model, proves better quality than Phi-3-mini, with a focus on high-quality, reasoning-dense data.",
    "model_family": "Microsoft",
    "model_version": "5",
    "notes": "## License\n\nThe model is licensed under the MIT license.\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "understanding"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "We report the results for Phi-3-Small-8K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mixtral-8x7b, Gemini-Pro, Gemma 7B, Llama-3-8B-Instruct, GPT-3.5-Turbo-1106, and GPT-4-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n|Benchmark|Phi-3-Small-8K-Instruct<br>7b|Gemma<br>7B|Mixtral<br>8x7B|Llama-3-Instruct<br>8b|GPT-3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|---------|-----------------------|--------|-------------|-------------------|-----------------|----------|------------------------|\n|AGI Eval<br>5-shot|45.1|42.1|45.2|42.0|48.4|49.0|59.6|\n|MMLU<br>5-shot|75.7|63.6|70.5|66.5|71.4|66.7|84.0|\n|BigBench Hard<br>3-shot|79.1|59.6|69.7|51.5|68.3|75.6|87.7|\n|ANLI<br>7-shot|58.1|48.7|55.2|57.3|58.1|64.2|71.7|\n|HellaSwag<br>5-shot|77.0|49.8|70.4|71.1|78.8|76.2|88.3|\n|ARC Challenge<br>10-shot|90.7|78.3|87.3|82.8|87.4|88.3|95.6|\n|ARC Easy<br>10-shot|97.0|91.4|95.6|93.4|96.3|96.1|98.8|\n|BoolQ<br>2-shot|84.8|66.0|76.6|80.9|79.1|86.4|91.3|\n|CommonsenseQA<br>10-shot|80.0|76.2|78.1|79.0|79.6|81.8|86.7|\n|MedQA<br>2-shot|65.4|49.6|62.2|60.5|63.4|58.2|83.7|\n|OpenBookQA<br>10-shot|88.0|78.6|85.8|82.6|86.0|86.4|93.4|\n|PIQA<br>5-shot|86.9|78.1|86.0|75.7|86.6|86.2|90.1|\n|Social IQA<br>5-shot|79.2|65.5|75.9|73.9|68.3|75.4|81.7|\n|TruthfulQA (MC2)<br>10-shot|70.2|52.1|60.1|63.2|67.7|72.6|85.2|\n|WinoGrande<br>5-shot|81.5|55.6|62.0|65.0|68.8|72.2|86.7|\n|TriviaQA<br>5-shot|58.1|72.3|82.2|67.7|85.8|80.2|73.3|\n|GSM8K Chain of Thought<br>8-shot|89.6|59.8|64.7|77.4|78.1|80.4|94.2|\n|HumanEval<br>0-shot|61.0|34.1|37.8|60.4|62.2|64.4|79.9|\n|MBPP<br>3-shot|71.7|51.5|60.2|67.7|77.8|73.2|86.7|\n|Average|75.7|61.8|69.8|69.4|74.3|75.4|85.2|\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n\n|Benchmark|Phi-3-Small-8K-Instruct<br>7b|Gemma<br>7B|Mixtral<br>8x7B|Llama-3-Instruct<br>8b|GPT-3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|--------|------------------------|--------|-------------|-------------------|-------------------|----------|------------------------|\n|Popular aggregated benchmark|71.1|59.4|66.2|59.9|67.0|67.5|80.5|\n|Reasoning|82.4|69.1|77.0|75.7|78.3|80.4|89.3|\n|Language understanding|70.6|58.4|64.9|65.4|70.4|75.3|81.6|\n|Code generation|60.7|45.6|52.7|56.4|70.4|66.7|76.1|\n|Math|51.6|35.8|40.3|41.1|52.8|50.9|67.1|\n|Factual knowledge|38.6|46.7|58.6|43.1|63.4|54.6|45.9|\n|Multilingual|62.5|63.2|63.4|65.0|69.1|76.5|82.0|\n|Robustness|72.9|38.4|51.0|64.5|69.3|69.7|84.6|",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-small-8k-instruct",
    "page": 2
  },
  {
    "id": "Phi-3-small-128k-instruct",
    "registry": "azureml",
    "name": "Phi-3-small instruct (128k)",
    "original_name": "Phi-3-small-128k-instruct",
    "friendly_name": "Phi-3-small instruct (128k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "The Phi-3-Small-128K-Instruct is a 7B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model supports 128K context length (in tokens).\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Small-128K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/phi3blog-april)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n\n## Model Architecture\n\nPhi-3 Small-128K-Instruct has 7B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).",
    "summary": "Same Phi-3-small model, but with a larger context size for RAG or few shot prompting.",
    "model_family": "Microsoft",
    "model_version": "5",
    "notes": "## License\n\nThe model is licensed under the MIT license.\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "understanding",
      "large context"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "We report the results for Phi-3-Small-128K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mixtral-8x7b, Gemini-Pro, Gemma 7B, Llama-3-8B-Instruct, GPT-3.5-Turbo-1106, and GPT-4-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n|Benchmark|Phi-3-Small-128K-Instruct<br>7b|Gemma<br>7B|Mixtral<br>8x7B|Llama-3-Instruct<br>8b|GPT-3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|---------|-------------------------------|----------|-------------|-------------------------|---------------------------|------------|--------------------------------|\n|AGI Eval<br>5-shot|43.9|42.1|45.2|42.0|48.4|49.0|59.6|\n|MMLU<br>5-shot|75.5|63.6|70.5|66.5|71.4|66.7|84.0|\n|BigBench Hard<br>3-shot|77.6|59.6|69.7|51.5|68.3|75.6|87.7|\n|ANLI<br>7-shot|55.8|48.7|55.2|57.3|58.1|64.2|71.7|\n|HellaSwag<br>5-shot|79.6|49.8|70.4|71.1|78.8|76.2|88.3|\n|ARC Challenge<br>10-shot|90.8|78.3|87.3|82.8|87.4|88.3|95.6|\n|ARC Easy<br>10-shot|97.3|91.4|95.6|93.4|96.3|96.1|98.8|\n|BoolQ<br>2-shot|83.7|66.0|76.6|80.9|79.1|86.4|91.3|\n|CommonsenseQA<br>10-shot|80.8|76.2|78.1|79.0|79.6|81.8|86.7|\n|MedQA<br>2-shot|46.3|49.6|62.2|60.5|63.4|58.2|83.7|\n|OpenBookQA<br>10-shot|87.8|78.6|85.8|82.6|86.0|86.4|93.4|\n|PIQA<br>5-shot|88.1|78.1|86.0|75.7|86.6|86.2|90.1|\n|Social IQA<br>5-shot|78.7|65.5|75.9|73.9|68.3|75.4|81.7|\n|TruthfulQA (MC2)<br>10-shot|69.6|52.1|60.1|63.2|67.7|72.6|85.2|\n|WinoGrande<br>5-shot|80.1|55.6|62.0|65.0|68.8|72.2|86.7|\n|TriviaQA<br>5-shot|66.0|72.3|82.2|67.7|85.8|80.2|73.3|\n|GSM8K Chain of Thought<br>8-shot|87.3|59.8|64.7|77.4|78.1|80.4|94.2|\n|HumanEval<br>0-shot|59.1|34.1|37.8|60.4|62.2|64.4|79.9|\n|MBPP<br>3-shot|70.3|51.5|60.2|67.7|77.8|73.2|86.7|\n|Average|74.6|61.8|69.8|69.4|74.3|75.4|85.2|\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n\n|Benchmark|Phi-3-Small-128K-Instruct<br>7b|Gemma<br>7B|Mixtral<br>8x7B|Llama-3-Instruct<br>8b|GPT-3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|--------|--------------------------|--------|-------------|-------------------|-------------------|----------|------------------------|\n|Popular aggregated benchmark|70.6|59.4|66.2|59.9|67.0|67.5|80.5|\n|Reasoning|80.3|69.1|77.0|75.7|78.3|80.4|89.3|\n|Language understanding|67.4|58.4|64.9|65.4|70.4|75.3|81.6|\n|Code generation|60.0|45.6|52.7|56.4|70.4|66.7|76.1|\n|Math|48.1|35.8|40.3|41.1|52.8|50.9|67.1|\n|Factual knowledge|41.7|46.7|58.6|43.1|63.4|54.6|45.9|\n|Multilingual|62.6|63.2|63.4|65.0|69.1|76.5|82.0|\n|Robustness|68.7|38.4|51.0|64.5|69.3|69.7|84.6|",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-small-128k-instruct",
    "page": 2
  },
  {
    "id": "Cohere-embed-v3-multilingual",
    "registry": "azureml-cohere",
    "name": "Cohere Embed v3 Multilingual",
    "original_name": "Cohere-embed-v3-multilingual",
    "friendly_name": "Cohere Embed v3 Multilingual",
    "task": "embeddings",
    "publisher": "Cohere",
    "license": "custom",
    "description": "Cohere Embed Multilingual is the market\u2019s leading multimodal (text, image) representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering. Embed Multilingual supports 100+ languages and can be used to search within a language (e.g., search with a French query on French documents) and across languages (e.g., search with an English query on Chinese documents). This model was trained on nearly 1B English training pairs and nearly 0.5B Non-English training pairs from 100+ languages.",
    "summary": "Cohere Embed Multilingual is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering.",
    "model_family": "Cohere",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "rag"
    ],
    "rate_limit_tier": "embeddings",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "it",
      "de",
      "pt-br",
      "ja",
      "ko",
      "zh-cn",
      "ar"
    ],
    "max_output_tokens": 0,
    "max_input_tokens": 512,
    "training_data_date": "",
    "evaluation": "Embed multilingual has SOTA performance on multilingual benchmarks such as Miracl and the multilingual evaluation results can be found in the following [Embed v3.0 Miracl Evaluation Results](https://github.com/cohere-ai/notebooks/blob/main/public_embedding_benchmarks/miracl.md) and full MTEB results can be found in the following [Embed v3.0 MTEB Evaluation Results](https://github.com/cohere-ai/notebooks/blob/main/public_embedding_benchmarks/mteb.md). Evaluations against multi-modal embedding models can be found in the following [Embed v3.0 Multimodal Evaluation Results](https://github.com/cohere-ai/notebooks/blob/main/public_embedding_benchmarks/multi_modal_evals.md).",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/cohere-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-cohere/Cohere-embed-v3-multilingual",
    "page": 2
  },
  {
    "id": "Cohere-embed-v3-english",
    "registry": "azureml-cohere",
    "name": "Cohere Embed v3 English",
    "original_name": "Cohere-embed-v3-english",
    "friendly_name": "Cohere Embed v3 English",
    "task": "embeddings",
    "publisher": "Cohere",
    "license": "custom",
    "description": "Cohere Embed English is the market\u2019s leading multimodal (text, image) representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering. Embed English has top performance on the HuggingFace MTEB benchmark and performs well on a variety of industries such as Finance, Legal, and General-Purpose Corpora.The model was trained on nearly 1B English training pairs.",
    "summary": "Cohere Embed English is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering.",
    "model_family": "Cohere",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "rag"
    ],
    "rate_limit_tier": "embeddings",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 0,
    "max_input_tokens": 512,
    "training_data_date": "",
    "evaluation": "Evaluation results can be found in the following [Embed v3.0 BEIR Evaluation Results](https://github.com/cohere-ai/notebooks/blob/main/public_embedding_benchmarks/beir.md) and full MTEB results can be found in the following [Embed v3.0 MTEB Evaluation Results](https://github.com/cohere-ai/notebooks/blob/main/public_embedding_benchmarks/mteb.md). Evaluations against multi-modal embedding models can be found in the following [Embed v3.0 Multimodal Evaluation Results](https://github.com/cohere-ai/notebooks/blob/main/public_embedding_benchmarks/multi_modal_evals.md).",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/cohere-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-cohere/Cohere-embed-v3-english",
    "page": 2
  },
  {
    "id": "text-embedding-3-small",
    "registry": "azure-openai",
    "name": "OpenAI Text Embedding 3 (small)",
    "original_name": "text-embedding-3-small",
    "friendly_name": "OpenAI Text Embedding 3 (small)",
    "task": "embeddings",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "Text-embedding-3 series models are the latest and most capable embedding model. The text-embedding-3 models offer better average multi-language retrieval performance with the MIRACL benchmark while still maintaining performance for English tasks with the MTEB benchmark.",
    "summary": "Text-embedding-3 series models are the latest and most capable embedding model from OpenAI.",
    "model_family": "OpenAI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions in are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Additional classification models and configuration options are available when you deploy an Azure OpenAI model in production; [learn more](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new). ",
    "tags": [
      "rag"
    ],
    "rate_limit_tier": "embeddings",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 0,
    "max_input_tokens": 8191,
    "training_data_date": "Sep 2021",
    "evaluation": "",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/text-embedding-3-small",
    "page": 2
  },
  {
    "id": "text-embedding-3-large",
    "registry": "azure-openai",
    "name": "OpenAI Text Embedding 3 (large)",
    "original_name": "text-embedding-3-large",
    "friendly_name": "OpenAI Text Embedding 3 (large)",
    "task": "embeddings",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "Text-embedding-3 series models are the latest and most capable embedding model. The text-embedding-3 models offer better average multi-language retrieval performance with the MIRACL benchmark while still maintaining performance for English tasks with the MTEB benchmark.",
    "summary": "Text-embedding-3 series models are the latest and most capable embedding model from OpenAI.",
    "model_family": "OpenAI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions in are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Additional classification models and configuration options are available when you deploy an Azure OpenAI model in production; [learn more](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new). ",
    "tags": [
      "rag"
    ],
    "rate_limit_tier": "embeddings",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 0,
    "max_input_tokens": 8191,
    "training_data_date": "Sep 2021",
    "evaluation": "",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/text-embedding-3-large",
    "page": 2
  },
  {
    "id": "gpt-4o-mini",
    "registry": "azure-openai",
    "name": "OpenAI GPT-4o mini",
    "original_name": "gpt-4o-mini",
    "friendly_name": "OpenAI GPT-4o mini",
    "task": "chat-completion",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).\n\nToday, GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens and knowledge up to October 2023. Thanks to the improved tokenizer shared with GPT-4o, handling non-English text is now even more cost effective.\n\nGPT-4o mini surpasses GPT-3.5 Turbo and other small models on academic benchmarks across both textual intelligence and multimodal reasoning, and supports the same range of languages as GPT-4o. It also demonstrates strong performance in function calling, which can enable developers to build applications that fetch data or take actions with external systems, and improved long-context performance compared to GPT-3.5 Turbo.\n\n## Resources\n\n- [OpenAI announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)",
    "summary": "An affordable, efficient AI solution for diverse text and image tasks.",
    "model_family": "OpenAI",
    "model_version": "2024-07-18",
    "notes": "## Model Provider\n\nThis model is provided through the Azure OpenAI service.\n\n## Relevant documents\n\nThe following documents are applicable:\n\n- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/legal/cognitive-services/openai/overview)\n- [Transparency Note for Azure OpenAI Service](https://learn.microsoft.com/legal/cognitive-services/openai/transparency-note)\n\n## Acknowledgments\n\n**Leads**: Jacob Menick, Kevin Lu, Shengjia Zhao, Eric Wallace, Hongyu Ren, Haitang Hu, Nick Stathas, Felipe Petroski Such\n\n**Program Lead**: Mianna Chen\n\nContributions noted in https://openai.com/gpt-4o-contributions/\n\n## Responsible AI Considerations\n\n**Built-in safety measures** - Safety is built into our models from the beginning, and reinforced at every step of our development process. In pre-training, we filter out information that we do not want our models to learn from or output, such as hate speech, adult content, sites that primarily aggregate personal information, and spam. In post-training, we align the model's behavior to our policies using techniques such as reinforcement learning with human feedback (RLHF) to improve the accuracy and reliability of the models' responses.\n\nGPT-4o mini has the same safety mitigations built-in as GPT-4o, which we carefully assessed using both automated and human evaluations according to our Preparedness Framework and in line with our voluntary commitments. More than 70 external experts in fields like social psychology and misinformation tested GPT-4o to identify potential risks, which we have addressed and plan to share the details of in the forthcoming GPT-4o system card and Preparedness scorecard. Insights from these expert evaluations have helped improve the safety of both GPT-4o and GPT-4o mini.\n\nBuilding on these learnings, our teams also worked to improve the safety of GPT-4o mini using new techniques informed by our research. GPT-4o mini in the API is the first model to apply our instruction hierarchy method, which helps to improve the model's ability to resist jailbreaks, prompt injections, and system prompt extractions. This makes the model's responses more reliable and helps make it safer to use in applications at scale.\n\nWe'll continue to monitor how GPT-4o mini is being used and improve the model's safety as we identify new risks.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Additional classification models and configuration options are available when you deploy an Azure OpenAI model in production; [learn more](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new). ",
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "it",
      "af",
      "es",
      "de",
      "fr",
      "id",
      "ru",
      "pl",
      "uk",
      "el",
      "lv",
      "zh",
      "ar",
      "tr",
      "ja",
      "sw",
      "cy",
      "ko",
      "is",
      "bn",
      "ur",
      "ne",
      "th",
      "pa",
      "mr",
      "te"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Oct 2023",
    "evaluation": "GPT-4o mini surpasses GPT-3.5 Turbo and other small models on academic benchmarks across both textual intelligence and multimodal reasoning, and supports the same range of languages as GPT-4o. It also demonstrates strong performance in function calling, which can enable developers to build applications that fetch data or take actions with external systems, and improved long-context performance compared to GPT-3.5 Turbo.\n\nGPT-4o mini has been evaluated across several key benchmarks.\n\n**Reasoning tasks**: GPT-4o mini is better than other small models at reasoning tasks involving both text and vision, scoring 82.0% on MMLU, a textual intelligence and reasoning benchmark, as compared to 77.9% for Gemini Flash and 73.8% for Claude Haiku.\n\n**Math and coding proficiency**: GPT-4o mini excels in mathematical reasoning and coding tasks, outperforming previous small models on the market. On MGSM, measuring math reasoning, GPT-4o mini scored 87.0%, compared to 75.5% for Gemini Flash and 71.7% for Claude Haiku. GPT-4o mini scored 87.2% on HumanEval, which measures coding performance, compared to 71.5% for Gemini Flash and 75.9% for Claude Haiku.  \n\n**Multimodal reasoning**: GPT-4o mini also shows strong performance on MMMU, a multimodal reasoning eval, scoring 59.4% compared to 56.1% for Gemini Flash and 50.2% for Claude Haiku.\n\n| Task                             | GPT-4o mini Score | Gemini Flash Score | Claude Haiku Score |\n|----------------------------------|-------------------|--------------------|--------------------|\n| MMLU (Reasoning Text and Vision) | 82.0%             | 77.9%              | 73.8%              |\n| MGSM (Math Reasoning)            | 87.0%             | 75.5%              | 71.7%              |\n| HumanEval (Coding Performance)   | 87.2%             | 71.5%              | 75.9%              |\n| MMMU (Multimodal Reasoning)      | 59.4%             | 56.1%              | 50.2%              |\n\nSource: [GPT-4o mini: advancing cost-efficient intelligence](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/).",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft\nProduct Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/gpt-4o-mini",
    "page": 2
  },
  {
    "id": "Cohere-command-r",
    "registry": "azureml-cohere",
    "name": "Cohere Command R",
    "original_name": "Cohere-command-r",
    "friendly_name": "Cohere Command R",
    "task": "chat-completion",
    "publisher": "Cohere",
    "license": "custom",
    "description": "Command R is a highly performant generative large language model, optimized for a variety of use cases including reasoning, summarization, and question answering. \n\nThe model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n\nPre-training data additionally included the following 13 languages: Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian.\n\n## Resources\n\nFor full details of this model, [release blog post](https://aka.ms/cohere-blog).\n\n## Model Architecture\n\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.\n\n### Tool use capabilities\n\nCommand R has been specifically trained with conversational tool use capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation.\n\nCommand R's tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R may use one of its supplied tools more than once.\n\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn't want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\n\n### Grounded Generation and RAG Capabilities\n\nCommand R has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG).This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\n\nCommand R's grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\n\nBy default, Command R will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\n\nThe model is trained with a number of other answering modes, which can be selected by prompt changes . A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\n\n### Code Capabilities\n\nCommand R has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.",
    "summary": "Command R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise.",
    "model_family": "Cohere",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "rag",
      "multilingual"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "it",
      "de",
      "pt-br",
      "ja",
      "ko",
      "zh-cn",
      "ar"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/cohere-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-cohere/Cohere-command-r",
    "page": 2
  },
  {
    "id": "Cohere-command-r-plus",
    "registry": "azureml-cohere",
    "name": "Cohere Command R+",
    "original_name": "Cohere-command-r-plus",
    "friendly_name": "Cohere Command R+",
    "task": "chat-completion",
    "publisher": "Cohere",
    "license": "custom",
    "description": "Command R+ is a highly performant generative large language model, optimized for a variety of use cases including reasoning, summarization, and question answering. \n\nThe model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n\nPre-training data additionally included the following 13 languages: Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian.\n\n## Resources\n\nFor full details of this model, [release blog post](https://aka.ms/cohere-blog).\n\n## Model Architecture\n\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.\n\n### Tool use capabilities\n\nCommand R+ has been specifically trained with conversational tool use capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation.\n\nCommand R+'s tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R+ may use one of its supplied tools more than once.\n\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn't want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\n\n### Grounded Generation and RAG Capabilities\n\nCommand R+ has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG).This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\n\nCommand R+'s grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\n\nBy default, Command R+ will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\n\nThe model is trained with a number of other answering modes, which can be selected by prompt changes . A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\n\n### Code Capabilities\n\nCommand R+ has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.",
    "summary": "Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads.",
    "model_family": "Cohere",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "rag",
      "multilingual"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "fr",
      "es",
      "it",
      "de",
      "pt-br",
      "ja",
      "ko",
      "zh-cn",
      "ar"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://aka.ms/cohere-azure-terms>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-cohere/Cohere-command-r-plus",
    "page": 2
  },
  {
    "id": "Mistral-large",
    "registry": "azureml-mistral",
    "name": "Mistral Large",
    "original_name": "Mistral-large",
    "friendly_name": "Mistral Large",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "Mistral Large is Mistral AI's most advanced Large Language Model (LLM). It can be used on any language-based task thanks to its state-of-the-art reasoning and knowledge capabilities.\n\nAdditionally, Mistral Large is:\n\n- **Specialized in RAG.** Crucial information is not lost in the middle of long context windows (up to 32K tokens).\n- **Strong in coding.**  Code generation, review and comments. Supports all mainstream coding languages.\n- **Multi-lingual by design.** Best-in-class performance in French, German, Spanish, and Italian - in addition to English. Dozens of other languages are supported.\n- **Responsible AI.** Efficient guardrails baked in the model, with additional safety layer with safe_mode option\n\n## Resources\n\nFor full details of this model, please read [release blog post](https://aka.ms/mistral-blog).",
    "summary": "Mistral's flagship model that's ideal for complex tasks that require large reasoning capabilities or are highly specialized (Synthetic Text Generation, Code Generation, RAG, or Agents).",
    "model_family": "Mistral AI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "rag",
      "agents",
      "multilingual"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "fr",
      "de",
      "es",
      "it",
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 32768,
    "training_data_date": "Mar 2023",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Mistral-large",
    "page": 2
  },
  {
    "id": "Mistral-small",
    "registry": "azureml-mistral",
    "name": "Mistral Small",
    "original_name": "Mistral-small",
    "friendly_name": "Mistral Small",
    "task": "chat-completion",
    "publisher": "Mistral AI",
    "license": "custom",
    "description": "Mistral Small is Mistral AI's most efficient Large Language Model (LLM). It can be used on any language-based task that requires high efficiency and low latency.\n\nMistral Small is:\n\n- **A small model optimized for low latency.**\u00a0Very efficient for high volume and low latency workloads. Mistral Small is Mistral's smallest proprietary model, it outperforms Mixtral 8x7B and has lower latency. \n- **Specialized in RAG.**\u00a0Crucial information is not lost in the middle of long context windows (up to 32K tokens).\n- **Strong in coding.**\u00a0Code generation, review and comments. Supports all mainstream coding languages.\n- **Multi-lingual by design.**\u00a0Best-in-class performance in French, German, Spanish, and Italian - in addition to English. Dozens of other languages are supported.\n- **Responsible AI.**\u00a0Efficient guardrails baked in the model, with additional safety layer with safe_mode option\n\n## Resources\n\nFor full details of this model, please read [release blog post](https://aka.ms/mistral-blog).",
    "summary": "Mistral Small can be used on any language-based task that requires high efficiency and low latency.",
    "model_family": "Mistral AI",
    "model_version": "1",
    "notes": "## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "low latency",
      "multilingual"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "fr",
      "de",
      "es",
      "it",
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 32768,
    "training_data_date": "Mar 2023",
    "evaluation": "",
    "license_description": "This model is provided under the License Terms available at <https://mistral.ai/terms-of-use/>.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-mistral/Mistral-small",
    "page": 2
  },
  {
    "id": "Meta-Llama-3-70B-Instruct",
    "registry": "azureml-meta",
    "name": "Meta-Llama-3-70B-Instruct",
    "original_name": "Meta-Llama-3-70B-Instruct",
    "friendly_name": "Meta-Llama-3-70B-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n## Model Architecture\n\nLlama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively. ",
    "summary": "A powerful 70-billion parameter model excelling in reasoning, coding, and broad language applications.",
    "model_family": "Meta",
    "model_version": "9",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n## License\n\nA custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\n## Intended Uses\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Enables applications to be Built with Meta Llama 3.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n\n## Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style=\"text-decoration:underline;\">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style=\"text-decoration:underline;\">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We've heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Testing against a benchmark combining CBRNE and adversarial intent, as well as fine tuning the model to help ensure it refuses to provide detailed information to promote potential CBRNE harm.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style=\"text-decoration:underline;\">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Meta's cybersecurity safety eval suite, measuring Llama 3's propensity to suggest insecure code when used as a coding assistant, and Llama 3's propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style=\"text-decoration:underline;\">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model's capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Responsible AI Considerations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
    "tags": [
      "conversation"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 8192,
    "training_data_date": "Dec 2023",
    "evaluation": "In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"6\" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n",
    "license_description": "A custom commercial license is available at <https://llama.meta.com/llama3/license>.\n",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Meta-Llama-3-70B-Instruct",
    "page": 2
  },
  {
    "id": "Meta-Llama-3-8B-Instruct",
    "registry": "azureml-meta",
    "name": "Meta-Llama-3-8B-Instruct",
    "original_name": "Meta-Llama-3-8B-Instruct",
    "friendly_name": "Meta-Llama-3-8B-Instruct",
    "task": "chat-completion",
    "publisher": "Meta",
    "license": "custom",
    "description": "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n## Model Architecture\n\nLlama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n## Training Datasets\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively. ",
    "summary": "A versatile 8-billion parameter model optimized for dialogue and text generation tasks.",
    "model_family": "Meta",
    "model_version": "9",
    "notes": "## Contact model provider\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n## License\n\nA custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\n## Intended Uses\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Enables applications to be Built with Meta Llama 3.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n\n## Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style=\"text-decoration:underline;\">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style=\"text-decoration:underline;\">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We've heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Testing against a benchmark combining CBRNE and adversarial intent, as well as fine tuning the model to help ensure it refuses to provide detailed information to promote potential CBRNE harm.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style=\"text-decoration:underline;\">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Meta's cybersecurity safety eval suite, measuring Llama 3's propensity to suggest insecure code when used as a coding assistant, and Llama 3's propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style=\"text-decoration:underline;\">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model's capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Responsible AI Considerations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
    "tags": [
      "conversation"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 8192,
    "training_data_date": "Mar 2023",
    "evaluation": "In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"6\" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n",
    "license_description": "A custom commercial license is available at <https://llama.meta.com/llama3/license>.\n",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml-meta/Meta-Llama-3-8B-Instruct",
    "page": 2
  },
  {
    "id": "Phi-3-mini-4k-instruct",
    "registry": "azureml",
    "name": "Phi-3-mini instruct (4k)",
    "original_name": "Phi-3-mini-4k-instruct",
    "friendly_name": "Phi-3-mini instruct (4k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n## Resources\n\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\ud83d\udee0\ufe0f [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3 Mini-4K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook - like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).",
    "summary": "Tiniest member of the Phi-3 family. Optimized for both quality and low latency.",
    "model_family": "Microsoft",
    "model_version": "15",
    "notes": "## License\n\nThe model is licensed under the MIT license.\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 4096,
    "training_data_date": "Oct 2023",
    "evaluation": "We report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7b | Mixtral-8x7b | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n|:----------|:-----------|:-------------------|:----------|:------------|:--------------|:----------------|:-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.0 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 70.9 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard CoT<br>3-shot| 73.5 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 53.6 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot| 75.3 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 86.3 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 78.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot| 56.5 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot| 82.2 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot| 83.5 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot| 30.6 | 2.9 | 15 | 6.9 | 32.4 | 30.8 |\n| | Social IQA <br>5-shot| 77.6 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot| 64.7 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot| 71.6 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot| 61.4 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoT <br>8-shot| 85.7 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot| 57.3 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot| 69.8 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **67.6** | **56.0** | **56.4** | **64.4** | **65.5** | **70.4** |\n\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-4K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 61.1 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 70.8 | 60.3 | 62.8 | 68.1 | 69.6 | 71.8 |\n| Language understanding | 60.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 60.7 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 50.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 38.4 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.7 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.   ",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-mini-4k-instruct",
    "page": 2
  },
  {
    "id": "Phi-3-mini-128k-instruct",
    "registry": "azureml",
    "name": "Phi-3-mini instruct (128k)",
    "original_name": "Phi-3-mini-128k-instruct",
    "friendly_name": "Phi-3-mini instruct (128k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.\nThis dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties.\n\nAfter initial training, the model underwent a post-training process that involved supervised fine-tuning and direct preference optimization to enhance its ability to follow instructions and adhere to safety measures.\nWhen evaluated against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning, the Phi-3 Mini-128K-Instruct demonstrated robust and state-of-the-art performance among models with fewer than 13 billion parameters.\n\n## Resources\n\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\ud83d\udee0\ufe0f [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook - like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).",
    "summary": "Same Phi-3-mini model, but with a larger context size for RAG or few shot prompting.",
    "model_family": "Microsoft",
    "model_version": "13",
    "notes": "## License\n\nThe model is licensed under the MIT license.\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Oct 2023",
    "evaluation": "We report the results under completion format for Phi-3-Mini-128K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT-3.5.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-128K-Ins | Gemma-7B | Mistral-7B | Mixtral-8x7B | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n| :----------| :-----------| :---------------------| :----------| :------------| :--------------| :----------------| :-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.5 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 69.7 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard <br>3-shot | 72.1 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 52.3 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot | 70.5 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 85.5 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 77.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot | 56.4 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot | 78.8 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot | 80.1 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot | 29.7 | 2.9 | 15 | 6.9 | 32.4 | 29.9 |\n| | Social IQA <br>5-shot | 74.7 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot | 64.8 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot | 71.0 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot | 57.8 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoTT <br>8-shot | 85.3 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot | 60.4 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot | 70.0 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **66.4** | **56.0** | **56.4** | **64.4** | **65.5** | **70.3** |\n\n**Long Context**: Phi-3 Mini-128K-Instruct supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA. \n\n| Benchmark     | Phi-3 Mini-128K-Instruct | Mistral-7B | Mixtral 8x7B | LLaMA-3-8B-Instruct |\n| :---------------| :--------------------------|:------------|:--------------|:---------------------|\n| GovReport     | 25.3                     | 4.9        | 20.3         | 10.3                |\n| QMSum         | 21.9                     | 15.5       | 20.6         | 2.9                 |\n| Qasper        | 41.6                     | 23.5       | 26.6         | 8.1                 |\n| SQuALITY      | 24.1                     | 14.7       | 16.2         | 25                  |\n| SummScreenFD  | 16.8                     | 9.3        | 11.3         | 5.1                 |\n| **Average**   | **25.9**                 | **13.6**   | **19.0**     | **10.3**            |\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-128K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:--------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 60.6 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 69.4 | 60.3 | 62.8 | 68.1 | 69.6 | 71.7 |\n| Language understanding | 57.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 61.0 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 51.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 35.8 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.4 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.   ",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-mini-128k-instruct",
    "page": 2
  },
  {
    "id": "Phi-3-medium-128k-instruct",
    "registry": "azureml",
    "name": "Phi-3-medium instruct (128k)",
    "original_name": "Phi-3-medium-128k-instruct",
    "friendly_name": "Phi-3-medium instruct (128k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "The Phi-3-Medium-128K-Instruct is a 14B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Medium version in two variants 4K and 128K which is the context length (in tokens) that it can support.\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Medium-128K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\ud83d\udee0\ufe0f [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3-Medium-128k-Instruct has 14B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook - like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).",
    "summary": "Same Phi-3-medium model, but with a larger context size for RAG or few shot prompting.",
    "model_family": "Microsoft",
    "model_version": "7",
    "notes": "## License\n\nThe model is licensed under the MIT license.\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "understanding",
      "large context"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 131072,
    "training_data_date": "Oct 2023",
    "evaluation": "We report the results for Phi-3-Medium-128k-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mixtral-8x22b, Gemini-Pro, Command R+ 104B, Llama-3-70B-Instruct, GPT-3.5-Turbo-1106, and GPT-4-Turbo-1106(Chat).\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n|Benchmark|Phi-3-Medium-128k-Instruct<br>14b|Command R+<br>104B|Mixtral<br>8x22B|Llama-3-70B-Instruct|GPT3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|---------|-----------------------|--------|-------------|-------------------|-------------------|----------|------------------------|\n|AGI Eval<br>5-shot|49.7|50.1|54.0|56.9|48.4|49.0|59.6|\n|MMLU<br>5-shot|76.6|73.8|76.2|80.2|71.4|66.7|84.0|\n|BigBench Hard<br>3-shot|77.9|74.1|81.8|80.4|68.3|75.6|87.7|\n|ANLI<br>7-shot|57.3|63.4|65.2|68.3|58.1|64.2|71.7|\n|HellaSwag<br>5-shot|81.6|78.0|79.0|82.6|78.8|76.2|88.3|\n|ARC Challenge<br>10-shot|91.0|86.9|91.3|93.0|87.4|88.3|95.6|\n|ARC Easy<br>10-shot|97.6|95.7|96.9|98.2|96.3|96.1|98.8|\n|BoolQ<br>2-shot|86.5|86.1|82.7|89.1|79.1|86.4|91.3|\n|CommonsenseQA<br>10-shot|82.2|82.0|82.0|84.4|79.6|81.8|86.7|\n|MedQA<br>2-shot|67.6|59.2|67.9|78.5|63.4|58.2|83.7|\n|OpenBookQA<br>10-shot|87.2|86.8|88.6|91.8|86.0|86.4|93.4|\n|PIQA<br>5-shot|87.8|86.4|85.0|85.3|86.6|86.2|90.1|\n|Social IQA<br>5-shot|79.0|75.3|78.2|81.1|68.3|75.4|81.7|\n|TruthfulQA (MC2)<br>10-shot|74.3|57.8|67.4|81.9|67.7|72.6|85.2|\n|WinoGrande<br>5-shot|78.9|77.0|75.3|83.3|68.8|72.2|86.7|\n|TriviaQA<br>5-shot|73.9|82.8|84.5|78.5|85.8|80.2|73.3|\n|GSM8K Chain of Thought<br>8-shot|87.5|78.3|83.8|93.5|78.1|80.4|94.2|\n|HumanEval<br>0-shot|58.5|61.6|39.6|78.7|62.2|64.4|79.9|\n|MBPP<br>3-shot|73.8|68.9|70.7|81.3|77.8|73.2|86.7|\n|Average|77.3|75.0|76.3|82.5|74.3|75.4|85.2|\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n\n|Benchmark|Phi-3-Medium-128k-Instruct<br>14b|Command R+<br>104B|Mixtral<br>8x22B|Llama-3-70B-Instruct|GPT3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|--------|------------------------|--------|-------------|-------------------|-------------------|----------|------------------------|\n| Popular aggregated benchmark | 72.3 | 69.9 | 73.4 | 76.3 | 67.0 | 67.5 | 80.5 |\n| Reasoning                    | 83.2 | 79.3 | 81.5 | 86.7 | 78.3 | 80.4 | 89.3 |\n| Language understanding       | 75.3 | 75.7 | 78.7 | 77.9 | 70.4 | 75.3 | 81.6 |\n| Code generation             | 64.2 | 68.6 | 60.0 | 69.3 | 70.4 | 66.7 | 76.1 |\n| Math                         | 52.9 | 45.3 | 52.5 | 59.7 | 52.8 | 50.9 | 67.1 |\n| Factual knowledge           | 47.5 | 60.3 | 60.6 | 52.4 | 63.4 | 54.6 | 45.9 |\n| Multilingual                | 62.2 | 67.8 | 69.8 | 62.0 | 67.0 | 73.4 | 78.2 |\n| Robustness                  | 70.2 | 57.9 | 65.5 | 78.7 | 69.3 | 69.7 | 84.6 |",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-medium-128k-instruct",
    "page": 3
  },
  {
    "id": "Phi-3-medium-4k-instruct",
    "registry": "azureml",
    "name": "Phi-3-medium instruct (4k)",
    "original_name": "Phi-3-medium-4k-instruct",
    "friendly_name": "Phi-3-medium instruct (4k)",
    "task": "chat-completion",
    "publisher": "Microsoft",
    "license": "mit",
    "description": "The Phi-3-Medium-4K-Instruct is a 14B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Medium version in two variants 4K and 128K which is the context length (in tokens) that it can support.\n\nThe model underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Medium-4K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\n\n## Resources\n\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n\ud83d\udee0\ufe0f [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\n## Model Architecture\n\nPhi-3-Medium-4K-Instruct has 14B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n## Training Datasets\n\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \"textbook-like\" data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).",
    "summary": "A 14B parameters model, proves better quality than Phi-3-mini, with a focus on high-quality, reasoning-dense data.",
    "model_family": "Microsoft",
    "model_version": "6",
    "notes": "## License\n\nThe model is licensed under the MIT license.\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202f[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview). Configuration options for content filtering vary when you deploy a model for production in Azure AI; [learn more](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview).",
    "tags": [
      "reasoning",
      "understanding"
    ],
    "rate_limit_tier": "low",
    "supported_languages": [
      "en"
    ],
    "max_output_tokens": 4096,
    "max_input_tokens": 4096,
    "training_data_date": "Oct 2023",
    "evaluation": "We report the results for Phi-3-Medium-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mixtral-8x22b, Gemini-Pro, Command R+ 104B, Llama-3-70B-Instruct, GPT-3.5-Turbo-1106, and GPT-4-Turbo-1106(Chat).\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n|Benchmark|Phi-3-Medium-4K-Instruct<br>14b|Command R+<br>104B|Mixtral<br>8x22B|Llama-3-70B-Instruct|GPT3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|---------|-----------------------|--------|-------------|-------------------|-------------------|----------|------------------------|\n|AGI Eval<br>5-shot|50.2|50.1|54.0|56.9|48.4|49.0|59.6|\n|MMLU<br>5-shot|78.0|73.8|76.2|80.2|71.4|66.7|84.0|\n|BigBench Hard<br>3-shot|81.4|74.1|81.8|80.4|68.3|75.6|87.7|\n|ANLI<br>7-shot|55.8|63.4|65.2|68.3|58.1|64.2|71.7|\n|HellaSwag<br>5-shot|82.4|78.0|79.0|82.6|78.8|76.2|88.3|\n|ARC Challenge<br>10-shot|91.6|86.9|91.3|93.0|87.4|88.3|95.6|\n|ARC Easy<br>10-shot|97.7|95.7|96.9|98.2|96.3|96.1|98.8|\n|BoolQ<br>2-shot|86.5|86.1|82.7|89.1|79.1|86.4|91.3|\n|CommonsenseQA<br>10-shot|82.8|82.0|82.0|84.4|79.6|81.8|86.7|\n|MedQA<br>2-shot|69.9|59.2|67.9|78.5|63.4|58.2|83.7|\n|OpenBookQA<br>10-shot|87.4|86.8|88.6|91.8|86.0|86.4|93.4|\n|PIQA<br>5-shot|87.9|86.4|85.0|85.3|86.6|86.2|90.1|\n|Social IQA<br>5-shot|80.2|75.3|78.2|81.1|68.3|75.4|81.7|\n|TruthfulQA (MC2)<br>10-shot|75.1|57.8|67.4|81.9|67.7|72.6|85.2|\n|WinoGrande<br>5-shot|81.5|77.0|75.3|83.3|68.8|72.2|86.7|\n|TriviaQA<br>5-shot|73.9|82.8|84.5|78.5|85.8|80.2|73.3|\n|GSM8K Chain of Thought<br>8-shot|91.0|78.3|83.8|93.5|78.1|80.4|94.2|\n|HumanEval<br>0-shot|62.2|61.6|39.6|78.7|62.2|64.4|79.9|\n|MBPP<br>3-shot|75.2|68.9|70.7|81.3|77.8|73.2|86.7|\n|Average|78.5|75.0|76.3|82.5|74.3|75.4|85.2|\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n\n|Benchmark|Phi-3-Medium-4K-Instruct<br>14b|Command R+<br>104B|Mixtral<br>8x22B|Llama-3-70B-Instruct|GPT3.5-Turbo<br>version 1106|Gemini<br>Pro|GPT-4-Turbo<br>version 1106 (Chat)|\n|--------|------------------------|--------|-------------|-------------------|-------------------|----------|------------------------|\n|Popular aggregated benchmark|75.4|69.9|73.4|76.3|67.0|67.5|80.5|\n|Reasoning|84.1|79.3|81.5|86.7|78.3|80.4|89.3|\n|Language understanding|73.9|75.6|78.1|76.9|68.7|76.2|80.7|\n|Code generation|66.1|68.6|60.0|69.3|70.4|66.7|76.1|\n|Math|52.8|45.3|52.5|59.7|52.8|50.9|67.1|\n|Factual knowledge|48.3|60.3|60.6|52.4|63.4|54.6|45.9|\n|Multilingual|62.9|67.8|69.8|62.0|67.0|73.4|78.2|\n|Robustness|66.5|57.9|65.5|78.7|69.3|69.7|84.6|",
    "license_description": "Microsoft.\nCopyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "static_model": false,
    "supported_input_modalities": [
      "text"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azureml/Phi-3-medium-4k-instruct",
    "page": 3
  },
  {
    "id": "gpt-4o",
    "registry": "azure-openai",
    "name": "OpenAI GPT-4o",
    "original_name": "gpt-4o",
    "friendly_name": "OpenAI GPT-4o",
    "task": "chat-completion",
    "publisher": "OpenAI",
    "license": "custom",
    "description": "gpt-4o offers a shift in how AI models interact with multimodal inputs. By seamlessly combining text, images, and audio, gpt-4o provides a richer, more engaging user experience.\n\nMatching the intelligence of gpt-4 turbo, it is remarkably more efficient, delivering text at twice the speed and at half the cost. Additionally, GPT-4o exhibits the highest vision performance and excels in non-English languages compared to previous OpenAI models.\n\ngpt-4o is engineered for speed and efficiency. Its advanced ability to handle complex queries with minimal resources can translate into cost savings and performance.\n\nThe introduction of gpt-4o opens numerous possibilities for businesses in various sectors:\n\n1. **Enhanced customer service**: By integrating diverse data inputs, gpt-4o enables more dynamic and comprehensive customer support interactions.\n2. **Advanced analytics**: Leverage gpt-4o's capability to process and analyze different types of data to enhance decision-making and uncover deeper insights.\n3. **Content innovation**: Use gpt-4o's generative capabilities to create engaging and diverse content formats, catering to a broad range of consumer preferences.\n\n## Updates\n\n`gpt-4o-2024-11-20`: this is the latest version of gpt-4o. Supports all previous output size (16,384) and features such as:  \n\n- Text, image processing\n- JSON Mode\n- parallel function calling\n- Enhanced accuracy and responsiveness\n- Parity with English text and coding tasks compared to GPT-4 Turbo with Vision\n- Superior performance in non-English languages and in vision tasks\n- Support for enhancements\n- Support for complex structured outputs.\n\n## Resources\n\n- [\"Hello gpt-4o\" (OpenAI announcement)](https://openai.com/index/hello-gpt-4o/)\n- [Introducing gpt-4o: OpenAI's new flagship multimodal model now in preview on Azure](https://azure.microsoft.com/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/)\n",
    "summary": "OpenAI's most advanced multimodal model in the gpt-4o family. Can handle both text and image inputs.",
    "model_family": "OpenAI",
    "model_version": "2024-11-20",
    "notes": "## Model Provider\n\nThis model is provided through the Azure OpenAI service.\n\n## Relevant documents\n\nThe following documents are applicable:\n\n- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/legal/cognitive-services/openai/overview)\n- [Transparency Note for Azure OpenAI Service](https://learn.microsoft.com/legal/cognitive-services/openai/transparency-note)\n\n## Responsible AI Considerations\n\ngpt-4o has safety built-in by design across modalities, through techniques such as filtering training data and refining the model's behavior through post-training. We have also created new safety systems to provide guardrails on voice outputs.\n\nWe've evaluated gpt-4o according to our [Preparedness Framework](https://openai.com/preparedness) and in line with our [voluntary commitments](https://openai.com/index/moving-ai-governance-forward/). Our evaluations of cybersecurity, CBRN, persuasion, and model autonomy show that GPT-4o does not score above Medium risk in any of these categories. This assessment involved running a suite of automated and human evaluations throughout the model training process. We tested both pre-safety-mitigation and post-safety-mitigation versions of the model, using custom fine-tuning and prompts, to better elicit model capabilities.\n\ngpt-4o has also undergone extensive external red teaming with 70+ [external experts](https://openai.com/index/red-teaming-network) in domains such as social psychology, bias and fairness, and misinformation to identify risks that are introduced or amplified by the newly added modalities. We used these learnings to build out our safety interventions in order to improve the safety of interacting with GPT-4o. We will continue to mitigate new risks as they're discovered.\n\nWe recognize that gpt-4o's audio modalities present a variety of novel risks. Today we are publicly releasing text and image inputs and text outputs. Over the upcoming weeks and months, we'll be working on the technical infrastructure, usability via post-training, and safety necessary to release the other modalities. For example, at launch, audio outputs will be limited to a selection of preset voices and will abide by our existing safety policies.\n\n## Content Filtering\n\nPrompts and completions are passed through a default configuration of Azure AI Content Safety classification models to detect and prevent the output of harmful content. Learn more about [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview). Additional classification models and configuration options are available when you deploy an Azure OpenAI model in production; [learn more](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new). ",
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "rate_limit_tier": "high",
    "supported_languages": [
      "en",
      "it",
      "af",
      "es",
      "de",
      "fr",
      "id",
      "ru",
      "pl",
      "uk",
      "el",
      "lv",
      "zh",
      "ar",
      "tr",
      "ja",
      "sw",
      "cy",
      "ko",
      "is",
      "bn",
      "ur",
      "ne",
      "th",
      "pa",
      "mr",
      "te"
    ],
    "max_output_tokens": 16384,
    "max_input_tokens": 131072,
    "training_data_date": "Oct 2023",
    "evaluation": "As measured on traditional benchmarks, gpt-4o achieves gpt-4 turbo-level performance on text, reasoning, and coding intelligence, while setting new high watermarks on multilingual, audio, and vision capabilities.\n\n| Model            | MMLU | GPQA | MATH | MGSM | DROP | HumanEval |\n|------------------|------|------|------|------|------|-----------|\n| GPT-4o (2024-08-06)           | 88.7 | 53.6 | 76.6 | 90.5 | 83.4 | 90.2      |\n| GPT-4T           | 86.5 | 48.0 | 72.6 | 88.5 | 86.0 | 87.1      |\n| GPT-4            | 86.4 | 35.7 | 42.5 | 74.5 | 80.9 | 67.0      |\n| Claude3 Opus     | 86.8 | 50.4 | 60.1 | 90.7 | 83.1 | 84.9      |\n| Gemini Pro 1.5   | 81.9 | --   | 58.5 | 88.7 | 78.9 | 71.9      |\n| Gemini Ultra 1.0 | 83.7 | --   | 53.2 | 79.0 | 82.4 | 74.4      |\n| Llama3 400b      | 86.1 | 48.0 | 57.8 | --   | 83.5 | 84.1      |\n\nSource: the [OpenAI announcement](https://openai.com/index/hello-gpt-4o/).\n",
    "license_description": "Use of Azure OpenAI Service is subject to applicable Microsoft Product Terms <https://www.microsoft.com/licensing/terms/welcome/welcomepage> including the Universal License Terms for Microsoft Generative AI Services and the service-specific terms for the Azure OpenAI product offering.",
    "static_model": false,
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "type": "model",
    "model_url": "/marketplace/models/azure-openai/gpt-4o",
    "page": 3
  }
]